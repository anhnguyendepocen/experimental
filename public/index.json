[{"authors":["lbelzile"],"categories":null,"content":"Léo Belzile is an assistant professor in the Department of Decision Sciences at HEC Montréal. His research focuses on extreme value analysis and its applications to environmental sciences.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"3a9b1b3cfa379001f2710addfc92cb80","permalink":"lbelzile.github.io/math807667a/authors/lbelzile/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"lbelzile.github.io/math807667a/authors/lbelzile/","section":"authors","summary":"Léo Belzile is an assistant professor in the Department of Decision Sciences at HEC Montréal. His research focuses on extreme value analysis and its applications to environmental sciences.","tags":null,"title":"","type":"authors"},{"authors":null,"categories":null,"content":"Each class session has a set of required readings that you should complete before watching the lecture.\nEvery class session also has a YouTube playlist of short recorded videos for each of the lecture sections. The lecture slides are special HTML files made with the R package xaringan. On each class session page you\u0026rsquo;ll see buttons for opening the presentation in a new tab or for downloading a PDF of the slides in case you want to print them or store them on your computer:\n View all slides in new window  Download PDF of all slides\nThe slides are also embedded on each page. You can click in the slides and navigate through them with ← and →. If you type ? (or shift + /) while viewing the slides you can see a list of slide-specific commands (like f for fullscreen or p for presenter mode if you want to see my notes).\n","date":1610323200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1610323200,"objectID":"8899c927408853efa5f455eaa551e047","permalink":"lbelzile.github.io/math807667a/content/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"lbelzile.github.io/math807667a/content/","section":"content","summary":"Each class session has a set of required readings that you should complete before watching the lecture.\nEvery class session also has a YouTube playlist of short recorded videos for each of the lecture sections.","tags":null,"title":"Readings, lectures, and videos","type":"docs"},{"authors":null,"categories":null,"content":"This section will contain annotated R code along with worked out examples. If time permits, I will also include videos of me life-coding, so you can see me making programming mistakes in real time!\nFor the time being, this section includes two useful pages by Dr. Andrew Heiss.\n","date":1604102400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1604102400,"objectID":"e9b55dc35cd7c0402d035e510f00bf75","permalink":"lbelzile.github.io/math807667a/example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"lbelzile.github.io/math807667a/example/","section":"example","summary":"This section will contain annotated R code along with worked out examples. If time permits, I will also include videos of me life-coding, so you can see me making programming mistakes in real time!","tags":null,"title":"Code examples","type":"docs"},{"authors":null,"categories":null,"content":"The main goals of this class are to help you design, critique, code, and run rigorous, valid, and feasible evaluations of public sector programs. Each type of assignment in this class is designed to help you achieve one or more of these goals.\nWeekly check-in Every week, after you finish working through the content, I want to hear about what you learned and what questions you still have. To facilitate this, and to encourage engagement with the course content, you\u0026rsquo;ll need to fill out a short response on iCollege. This should be ≈150 words. That\u0026rsquo;s fairly short: there are ≈250 words on a typical double-spaced page in Microsoft Word (500 when single-spaced).\nYou should answer these two questions each week:\n What was the most exciting thing you learned from the session? Why? What was the muddiest thing from the session this week? What are you still wondering about?  I will grade these check-ins using a check system:\n ✔+: (11.5 points (115%) in gradebook) Response shows phenomenal thought and engagement with the course content. I will not assign these often. ✔: (10 points (100%) in gradebook) Response is thoughtful, well-written, and shows engagement with the course content. This is the expected level of performance. ✔−: (5 points (50%) in gradebook) Response is hastily composed, too short, and/or only cursorily engages with the course content. This grade signals that you need to improve next time. I will hopefully not assign these often.  Notice that is essentially a pass/fail or completion-based system. I\u0026rsquo;m not grading your writing ability, I\u0026rsquo;m not counting the exact number of words you\u0026rsquo;re writing, and I\u0026rsquo;m not looking for encyclopedic citations of every single reading to prove that you did indeed read everything. I\u0026rsquo;m looking for thoughtful engagement, that\u0026rsquo;s all. Do good work and you\u0026rsquo;ll get a ✓.\nYou will submit these responses via iCollege.\nProblem sets To practice writing R code, running inferential models, and thinking about causation, you will complete a series of problem sets.\nThere are 9 problem sets on the schedule. I will keep the highest grades for 8 of them. That is, I will drop the lowest score (even if it\u0026rsquo;s a zero). This means you can skip one of the problem sets. You need to show that you made a good faith effort to work each question. I will not grade these in detail. The problem sets will be graded using a check system:\n ✔+: (33 points (110%) in gradebook) Assignment is 100% completed. Every question was attempted and answered, and most answers are correct. Document is clean and easy to follow. Work is exceptional. I will not assign these often. ✔: (30 points (100%) in gradebook) Assignment is 70–99% complete and most answers are correct. This is the expected level of performance. ✔−: (15 points (50%) in gradebook) Assignment is less than 70% complete and/or most answers are incorrect. This indicates that you need to improve next time. I will hopefully not asisgn these often.  You may (and should!) work together on the problem sets, but you must turn in your own answers. You cannot work in groups of more than four people, and you must note who participated in the group in your assignment.\nEvaluation assignments For your final project, you will conduct a pre-registered evaluation of a social program using synthetic data. To (1) give you practice with the principles of program evaluation, research design, measurement, and causal diagrams, and (2) help you with the foundation of your final project, you will complete a set of four evaluation-related assignments.\nIdeally these will become major sections of your final project. However, there is no requirement that the programs you use in these assignments must be the same as the final project. If, through these assignments, you discover that your initially chosen program is too simple, too complex, too boring, etc., you can change at any time.\nThese assignments will be graded using a check system:\n ✔+: (33 points (110%) in gradebook) Assignment is 100% completed. Every question was attempted and answered, and most answers are correct. Document is clean and easy to follow. Work is exceptional. I will not assign these often. ✔: (30 points (100%) in gradebook) Assignment is 70–99% complete and most answers are correct. This is the expected level of performance. ✔−: (15 points (50%) in gradebook) Assignment is less than 70% complete and/or most answers are incorrect. This indicates that you need to improve next time. I will hopefully not asisgn these often.  Exams There will be two exams covering (1) program evaluation, design, and causation, and (2) the core statistical tools of program evaluation and causal inference.\nYou will take these exams online through iCollege. The exams will have a time limit, but you can use notes and readings and the Google. You must take the exams on your own though, and not talk to anyone about them.\nFinal project At the end of the course, you will demonstrate your knowledge of program evaluation and causal inference by completing a final project.\nComplete details for the final project are here.\nThere is no final exam. This project is your final exam.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"e18c399687bc0897ffd6503c7a1bbb8e","permalink":"lbelzile.github.io/math807667a/assignment/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"lbelzile.github.io/math807667a/assignment/","section":"assignment","summary":"The main goals of this class are to help you design, critique, code, and run rigorous, valid, and feasible evaluations of public sector programs. Each type of assignment in this class is designed to help you achieve one or more of these goals.","tags":null,"title":"Assignment details","type":"docs"},{"authors":null,"categories":null,"content":"Content  Syllabus and learning objectives  Programming premises   Motivational  Examples of experiments Notions of causal inference   Differences between observational and experimental studies Key concepts in experimental designs  Randomization Blocking    Learning objectives At the end of the session, students should be capable of\n distinguishing between observational and experimental studies, and the merits of both. identifying experimental units, observations and treatments in an experimental study assessing the generalizability of a study using R for basic manipulations  Preliminaries   Familiarize yourself with the syllabus, content, examples, and assignments pages for this class.\n   Read Chapter 1 (Intro to Data) of OpenIntro Statistics and the accompanying videos\n  Overview of R and the tidyverse slides by Andrew Heiss)\n  Readings   Abstract of Hariton, E and J.J. Locascio (2018), Randomised controlled trials – the gold standard for effectiveness research  Chapter 1 (*Preliminaries) in *Planning of experiments*1  Chapter 1 and Sections 2.1-2.2 in Design and Analysis of Experiments2  Examples from Section 1.4 of Experimental Design3  Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nIntroduction  Class details  Motivation  Experimental vs observational  Key notions                   Fun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.  Videos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction Class details Motivation Experimental vs observational Key notions  You can also watch the playlist (and skip around to different sections) here:\n    David R. Cox, Planning of Experiments (New York, NY: Wiley, 1958).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n A. Dean, D. Voss, and D. Draguljić, Design and Analysis of Experiments (Springer, 2017), https://www.springer.com/gp/book/9783319522487.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Paul D. Berger, Robert E. Maurer, and Giovana B. Celli, Experimental Design with Applications in Management, Engineering, and the Sciences, 2nd ed. (Springer, 2018), doi:10.1007/978-3-319-64583-4.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","date":1610323200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610323200,"objectID":"24bdef858b9ebc83bb0134c283e06cf1","permalink":"lbelzile.github.io/math807667a/content/01-content/","publishdate":"2021-01-11T00:00:00Z","relpermalink":"lbelzile.github.io/math807667a/content/01-content/","section":"content","summary":"Content  Syllabus and learning objectives  Programming premises   Motivational  Examples of experiments Notions of causal inference   Differences between observational and experimental studies Key concepts in experimental designs  Randomization Blocking    Learning objectives At the end of the session, students should be capable of","tags":null,"title":"Introduction to experimental designs","type":"docs"},{"authors":null,"categories":null,"content":" Generating fake or simulated data is an incredibly powerful skill, but it takes some practice. Here are a bunch of helpful resources and code examples of how to use different R functions to generate random numbers that follow specific distributions (or probability shapes).\nThis example focuses primarily on distributions. Each of the columns you\u0026rsquo;ll generate will be completely independent from each other and there will be no correlation between them. The example for generating synthetic data provides code and a bunch of examples of how to build in correlations between columns.\nFirst, make sure you load the libraries we\u0026rsquo;ll use throughout the example:\nlibrary(tidyverse) library(patchwork)  Seeds When R (or any computer program, really) generates random numbers, it uses an algorithm to simulate randomness. This algorithm always starts with an initial number, or seed. Typically it will use something like the current number of milliseconds since some date, so that every time you generate random numbers they\u0026rsquo;ll be different. Look at this, for instance:\n# Choose 3 numbers between 1 and 10 sample(1:10, 3) ## [1] 9 4 7  # Choose 3 numbers between 1 and 10 sample(1:10, 3) ## [1] 5 6 9  They\u0026rsquo;re different both times.\nThat\u0026rsquo;s ordinarily totally fine, but if you care about reproducibility (like having a synthetic dataset with the same random values, or having jittered points in a plot be in the same position every time you knit), it\u0026rsquo;s a good idea to set your own seed. This ensures that the random numbers you generate are the same every time you generate them.\nDo this by feeding set.seed() some numbers. It doesn\u0026rsquo;t matter what number you use—it just has to be a whole number. People have all sorts of favorite seeds:\n 1 13 42 1234 12345 20201101 (i.e. the current date) 8675309  You could even go to random.org and use atmospheric noise to generate a seed, and then use that in R.\nHere\u0026rsquo;s what happens when you generate random numbers after setting a seed:\n# Set a seed set.seed(1234) # Choose 3 numbers between 1 and 10 sample(1:10, 3) ## [1] 10 6 5 # Set a seed set.seed(1234) # Choose another 3 numbers between 1 and 10 sample(1:10, 3) ## [1] 10 6 5  They\u0026rsquo;re the same!\nOnce you set a seed, it influences any function that does anything random, but it doesn\u0026rsquo;t reset. For instance, if you set a seed once and then run sample() twice, you\u0026rsquo;ll get different numbers the second time, but you\u0026rsquo;ll get the same different numbers every time:\n# Set a seed set.seed(1234) # Choose 3 numbers between 1 and 10 sample(1:10, 3) ## [1] 10 6 5 sample(1:10, 3) # This will be different! ## [1] 9 5 6 # Set a seed again set.seed(1234) # Choose 3 numbers between 1 and 10 sample(1:10, 3) ## [1] 10 6 5 sample(1:10, 3) # This will be different, but the same as before! ## [1] 9 5 6  Typically it\u0026rsquo;s easiest to just include set.seed(SOME_NUMBER) at the top of your script after you load all the libraries. Some functions have a seed argument, and it\u0026rsquo;s a good idea to use it: position_jitter(..., seed = 1234).\nDistributions Remember in elementary school when you\u0026rsquo;d decide on playground turns by saying \u0026ldquo;Pick a number between 1 and 10\u0026rdquo; and whoever was the closest would win? When you generate random numbers in R, you\u0026rsquo;re essentially doing the same thing, only with some fancier bells and whistles.\nWhen you ask someone to choose a number between 1 and 10, any of those numbers should be equally likely. 1 isn\u0026rsquo;t really less common than 5 or anything. In some situations, though, there are numbers that are more likely to appear than others (i.e. when you roll two dice, it\u0026rsquo;s pretty rare to get a 2, but pretty common to get a 7). These different kinds of likelihood change the shape of the distribution of possible values. There are hundreds of different distributions, but for the sake of generating data, there are only a few that you need to know.\nUniform distribution In a uniform distribution, every number is equally likely. This is the \u0026ldquo;pick a number between 1 and 10\u0026rdquo; scenario, or rolling a single die. There are a couple ways to work with a uniform distribution in R: (1) sample() and (2) runif().\nsample() The sample() function chooses an element from a list.\nFor instance, let\u0026rsquo;s pretend we have six possible numbers (like a die, or like 6 categories on a survey), like this:\npossible_answers \u0026lt;- c(1, 2, 3, 4, 5, 6) # We could also write this as 1:6 instead  If we want to randomly choose from this list, you\u0026rsquo;d use sample(). The size argument defines how many numbers to choose.\n# Choose 1 random number sample(possible_answers, size = 1) ## [1] 4 # Choose 3 random numbers sample(possible_answers, size = 3) ## [1] 2 6 5  One important argument you can use is replace, which essentially puts the number back into the pool of possible numbers. Imagine having a bowl full of ping pong balls with the numbers 1–6 on them. If you take the number \u0026ldquo;3\u0026rdquo; out, you can\u0026rsquo;t draw it again. If you put it back in, you can pull it out again. The replace argument puts the number back after it\u0026rsquo;s drawn:\n# Choose 10 random numbers, with replacement sample(possible_answers, size = 10, replace = TRUE) ## [1] 6 4 6 6 6 4 4 5 4 3  If you don\u0026rsquo;t specify replace = TRUE, and you try to choose more numbers than are in the set, you\u0026rsquo;ll get an error:\n# Choose 8 numbers between 1 and 6, but don't replace them. # This won't work! sample(possible_answers, size = 8) ## Error in sample.int(length(x), size, replace, prob): impossible de prendre un échantillon plus grand que la population lorsque 'replace = FALSE'  It\u0026rsquo;s hard to see patterns in the outcomes when generating just a handful of numbers, but easier when you do a lot. Let\u0026rsquo;s roll a die 1,000 times:\nset.seed(1234) die \u0026lt;- tibble(value = sample(possible_answers, size = 1000, replace = TRUE)) die %\u0026gt;% count(value) ## # A tibble: 6 × 2 ## value n ## \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 1 1 161 ## 2 2 153 ## 3 3 188 ## 4 4 149 ## 5 5 157 ## 6 6 192 ggplot(die, aes(x = value)) + geom_bar() + labs(title = \u0026quot;1,000 rolls of a single die\u0026quot;)  In this case, 3 and 6 came up more often than the others, but that\u0026rsquo;s just because of randomness. If we rolled the die 100,000 times, the bars should basically be the same:\nset.seed(1234) die \u0026lt;- tibble(value = sample(possible_answers, size = 100000, replace = TRUE)) ggplot(die, aes(x = value)) + geom_bar() + labs(title = \u0026quot;100,000 rolls of a single die\u0026quot;)  runif() Another way to generate uniformly distributed numbers is to use the runif() function (which is short for \u0026ldquo;random uniform\u0026rdquo;, and which took me years to realize, and for years I wondered why people used a function named \u0026ldquo;run if\u0026rdquo; when there\u0026rsquo;s no if statement anywhere??)\nrunif() will choose numbers between a minimum and a maximum. These numbers will not be whole numbers. By default, the min and max are 0 and 1:\nrunif(5) ## [1] 0.09862 0.96294 0.88655 0.05623 0.44452  Here are 5 numbers between 35 and 56:\nrunif(5, min = 35, max = 56) ## [1] 46.83 42.89 37.75 53.22 46.13  Since these aren\u0026rsquo;t whole numbers, you can round them to make them look more realistic (like, if you were generating a column for age, you probably don\u0026rsquo;t want people who are 21.5800283 years old):\n# Generate 5 people between the ages of 18 and 35 round(runif(5, min = 18, max = 35), 0) ## [1] 21 28 33 34 31  You can confirm that each number has equal probability if you make a histogram. Here are 5,000 random people between 18 and 35:\nset.seed(1234) lots_of_numbers \u0026lt;- tibble(x = runif(5000, min = 18, max = 35)) ggplot(lots_of_numbers, aes(x = x)) + geom_histogram(binwidth = 1, color = \u0026quot;white\u0026quot;, boundary = 18)  Normal distribution The whole \u0026ldquo;choose a number between 1 and 10\u0026rdquo; idea of a uniform distribution is neat and conceptually makes sense, but most numbers that exist in the world tend to have higher probabilities around certain values—almost like gravity around a specific point. For instance, income in the United States is not uniformly distributed—a handful of people are really really rich, lots are very poor, and most are kind of clustered around an average.\nThe idea of having possible values clustered around an average is how the rest of these distributions work (uniform distributions don\u0026rsquo;t have any sort of central gravity point; all these others do). Each distribution is defined by different things called parameters, or values that determine the shape of the probabilities and locations of the clusters.\nA super common type of distribution is the normal distribution. This is the famous \u0026ldquo;bell curve\u0026rdquo; you learn about in earlier statistics classes. A normal distribution has two parameters:\n A mean (the center of the cluster) A standard deviation (how much spread there is around the mean).  In R, you can generate random numbers from a normal distribution with the rnorm() function. It takes three arguments: the number of numbers you want to generate, the mean, and the standard deviation. It defaults to a mean of 0 and a standard deviation of 1, which means most numbers will cluster around 0, with a lot between −1 and 1, and some going up to −2 and 2 (technically 67% of numbers will be between −1 and 1, while 95% of numbers will be between −2–2ish)\nrnorm(5) ## [1] -1.3662 0.5392 -1.3219 -0.2813 -2.1049 # Cluster around 10, with an SD of 4 rnorm(5, mean = 10, sd = 4) ## [1] 3.530 7.105 11.227 10.902 13.743  When working with uniform distributions, it\u0026rsquo;s easy to know how high or low your random values might go, since you specify a minimum and maximum number. With a normal distribution, you don\u0026rsquo;t specify starting and ending points—you specify a middle and a spread, so it\u0026rsquo;s harder to guess the whole range. Plotting random values is thus essential. Here\u0026rsquo;s 1,000 random numbers clustered around 10 with a standard deviation of 4:\nset.seed(1234) plot_data \u0026lt;- tibble(x = rnorm(1000, mean = 10, sd = 4)) head(plot_data) ## # A tibble: 6 × 1 ## x ## \u0026lt;dbl\u0026gt; ## 1 5.17 ## 2 11.1 ## 3 14.3 ## 4 0.617 ## 5 11.7 ## 6 12.0 ggplot(plot_data, aes(x = x)) + geom_histogram(binwidth = 1, boundary = 0, color = \u0026quot;white\u0026quot;)  Neat. Most numbers are around 10; lots are between 5 and 15; some go as high as 25 and as low as −5.\nWatch what happens if you change the standard deviation to 10 to make the spread wider:\nset.seed(1234) plot_data \u0026lt;- tibble(x = rnorm(1000, mean = 10, sd = 10)) head(plot_data) ## # A tibble: 6 × 1 ## x ## \u0026lt;dbl\u0026gt; ## 1 -2.07 ## 2 12.8 ## 3 20.8 ## 4 -13.5 ## 5 14.3 ## 6 15.1 ggplot(plot_data, aes(x = x)) + geom_histogram(binwidth = 1, boundary = 0, color = \u0026quot;white\u0026quot;)  It\u0026rsquo;s still centered around 10, but now you get values as high as 40 and as low as −20. The data is more spread out now.\nWhen simulating data, you\u0026rsquo;ll most often use a normal distribution just because it\u0026rsquo;s easy and lots of things follow that pattern in the real world. Incomes, ages, education, etc. all have a kind of gravity to them, and a normal distribution is a good way of showing that gravity. For instance, here are 1,000 simulated people with reasonable random incomes, ages, and years of education:\nset.seed(1234) fake_people \u0026lt;- tibble(income = rnorm(1000, mean = 40000, sd = 15000), age = rnorm(1000, mean = 25, sd = 8), education = rnorm(1000, mean = 16, sd = 4)) head(fake_people) ## # A tibble: 6 × 3 ## income age education ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 21894. 15.4 12.1 ## 2 44161. 27.4 15.6 ## 3 56267. 12.7 15.6 ## 4 4815. 30.1 20.8 ## 5 46437. 30.6 9.38 ## 6 47591. 9.75 11.8 fake_income \u0026lt;- ggplot(fake_people, aes(x = income)) + geom_histogram(binwidth = 5000, color = \u0026quot;white\u0026quot;, boundary = 0) + labs(title = \u0026quot;Simulated income\u0026quot;) fake_age \u0026lt;- ggplot(fake_people, aes(x = age)) + geom_histogram(binwidth = 2, color = \u0026quot;white\u0026quot;, boundary = 0) + labs(title = \u0026quot;Simulated age\u0026quot;) fake_education \u0026lt;- ggplot(fake_people, aes(x = education)) + geom_histogram(binwidth = 2, color = \u0026quot;white\u0026quot;, boundary = 0) + labs(title = \u0026quot;Simulated education\u0026quot;) fake_income + fake_age + fake_education  These three columns all have different centers and spreads. Income is centered around $45,000, going up to almost $100,000 and as low as −$10,000; age is centered around 25, going as low as 0 and as high as 50; education is centered around 16, going as low as 3 and as high as 28. Cool.\nAgain, when generating these numbers, it\u0026rsquo;s really hard to know how high or low these ranges will be, so it\u0026rsquo;s a good idea to plot them constantly. I settled on sd = 4 for education only because I tried things like 1 and 10 and got wild looking values (everyone basically at 16 with little variation, or everyone ranging from −20 to 50, which makes no sense when thinking about years of education). Really it\u0026rsquo;s just a process of trial and error until the data looks good and reasonable.\nTruncated normal distribution Sometimes you\u0026rsquo;ll end up with negative numbers that make no sense. Look at income in the plot above, for instance. Some people are earning −$10,000 year. The rest of the distribution looks okay, but those negative values are annoying.\nTo fix this, you can use something called a truncated normal distribution, which lets you specify a mean and standard deviation, just like a regular normal distribution, but also lets you specify a minimum and/or maximum so you don\u0026rsquo;t get values that go too high or too low.\nR doesn\u0026rsquo;t have a truncated normal function built-in, but you can install the truncnorm package and use the rtruncnorm() function. A truncated normal distribution has four parameters:\n A mean (mean) A standard deviation (sd) A minimum (optional) (a) A maximum (optional) (b)  For instance, let\u0026rsquo;s pretend you have a youth program designed to target people who are between 12 and 21 years old, with most around 14. You can generate numbers with a mean of 14 and a standard deviation of 5, but you\u0026rsquo;ll create people who are too old, too young, or even negatively aged!\nset.seed(1234) plot_data \u0026lt;- tibble(fake_age = rnorm(1000, mean = 14, sd = 5)) head(plot_data) ## # A tibble: 6 × 1 ## fake_age ## \u0026lt;dbl\u0026gt; ## 1 7.96 ## 2 15.4 ## 3 19.4 ## 4 2.27 ## 5 16.1 ## 6 16.5 ggplot(plot_data, aes(x = fake_age)) + geom_histogram(binwidth = 2, color = \u0026quot;white\u0026quot;, boundary = 0)  To fix this, truncate the range at 12 and 21:\nlibrary(truncnorm) # For rtruncnorm() set.seed(1234) plot_data \u0026lt;- tibble(fake_age = rtruncnorm(1000, mean = 14, sd = 5, a = 12, b = 21)) head(plot_data) ## # A tibble: 6 × 1 ## fake_age ## \u0026lt;dbl\u0026gt; ## 1 15.4 ## 2 19.4 ## 3 16.1 ## 4 16.5 ## 5 14.3 ## 6 18.8 ggplot(plot_data, aes(x = fake_age)) + geom_histogram(binwidth = 1, color = \u0026quot;white\u0026quot;, boundary = 0)  And voila! A bunch of people between 12 and 21, with most around 14, with no invalid values.\nBeta distribution Normal distributions are neat, but they\u0026rsquo;re symmetrical around the mean (unless you truncate them). What if your program involves a test with a maximum of 100 points where most people score around 85, but a sizable portion score below that. In other words, it\u0026rsquo;s not centered at 85, but is skewed left.\nTo simulate this kind of distribution, we can use a Beta distribution. Beta distributions are neat because they naturally only range between 0 and 1—they\u0026rsquo;re perfect for things like percentages or proportions or or 100-based exams.\nUnlike a normal distribution, where you use the mean and standard deviation as parameters, Beta distributions take two non-intuitive parameters:\n shape1 shape2  What the heck are these shapes though?! This answer at Cross Validated does an excellent job of explaining the intuition behind Beta distributions and it\u0026rsquo;d be worth it to read it.\nBasically, Beta distributions are good at modeling probabilities of things, and shape1 and shape2 represent specific parts of a probability formula.\nLet\u0026rsquo;s say that there\u0026rsquo;s an exam with 10 points where most people score a 6/10. Another way to think about this is that an exam is a collection of correct answers and incorrect answers, and that the percent correct follows this equation:\n$$ \\frac{\\text{Number correct}}{\\text{Number correct} + \\text{Number incorrect}} $$\nIf you scored a 6, you could write that as:\n$$ \\frac{6}{6 + 4} $$\nTo make it more general, we can use Greek variable names: \\(\\alpha\\) for the number correct and \\(\\beta\\) for the number incorrect, leaving us with this:\n$$ \\frac{\\alpha}{\\alpha + \\beta} $$\nNeat.\nIn a Beta distribution, the \\(\\alpha\\) and \\(\\beta\\) in that equation correspond to shape1 and shape2. If we want to generate random scores for this test where most people get 6/10, we can use rbeta():\nset.seed(1234) plot_data \u0026lt;- tibble(exam_score = rbeta(1000, shape1 = 6, shape2 = 4)) %\u0026gt;% # rbeta() generates numbers between 0 and 1, so multiply everything by 10 to # scale up the exam scores mutate(exam_score = exam_score * 10) ggplot(plot_data, aes(x = exam_score)) + geom_histogram(binwidth = 1, color = \u0026quot;white\u0026quot;) + scale_x_continuous(breaks = 0:10)  Most people score around 6, with a bunch at 5 and 7, and fewer in the tails. Importantly, it\u0026rsquo;s not centered at 6—the distribution is asymmetric.\nThe magic of—and most confusing part about—Beta distributions is that you can get all sorts of curves by just changing the shape parameters. To make this easier to see, we can make a bunch of different Beta distributions. Instead of plotting them with histograms, we\u0026rsquo;ll use density plots (and instead of generating random numbers, we\u0026rsquo;ll plot the actual full range of the distribution (that\u0026rsquo;s what dbeta and geom_function() do in all these examples)).\nHere\u0026rsquo;s what we saw before, with \\(\\alpha\\) (shape1) = 6 and \\(\\beta\\) (shape2) = 4:\nggplot() + geom_function(fun = dbeta, args = list(shape1 = 6, shape2 = 4))  Again, there\u0026rsquo;s a peak at 0.6 (or 6), which is what we expected.\nWe can make the distribution narrower if we scale the shapes up. Here pretty much everyone scores around 50% and 75%.\nggplot() + geom_function(fun = dbeta, args = list(shape1 = 60, shape2 = 40))  So far all these curves look like normal distributions, just slightly skewed. But when if most people score 90–100%? Or most fail? A Beta distribution can handle that too:\nggplot() + geom_function(fun = dbeta, args = list(shape1 = 9, shape2 = 1), color = \u0026quot;blue\u0026quot;) + geom_function(fun = dbeta, args = list(shape1 = 1, shape2 = 9), color = \u0026quot;red\u0026quot;)  With shape1 = 9 and shape2 = 1 (or \\(\\frac{9}{9 + 1}\\)) we get most around 90%, while shape1 = 1 and shape2 = 9 (or \\(\\frac{1}{1 + 9}\\)) gets us most around 10%.\nCheck out all these other shapes too:\nggplot() + geom_function(fun = dbeta, args = list(shape1 = 5, shape2 = 5), color = \u0026quot;blue\u0026quot;) + geom_function(fun = dbeta, args = list(shape1 = 2, shape2 = 5), color = \u0026quot;red\u0026quot;) + geom_function(fun = dbeta, args = list(shape1 = 80, shape2 = 23), color = \u0026quot;orange\u0026quot;) + geom_function(fun = dbeta, args = list(shape1 = 13, shape2 = 17), color = \u0026quot;brown\u0026quot;)  In real life, if I don\u0026rsquo;t want to figure out the math behind the \\(\\frac{\\alpha}{\\alpha + \\beta}\\) shape values, I end up just choosing different numbers until it looks like the shape I want, and then I use rbeta() with those parameter values. Like, how about we generate some numbers based on the red line above, with shape1 = 2 and shape2 = 5, which looks like it should be centered around 0.2ish ($\\frac{2}{2 + 5} = 0.2857$):\nset.seed(1234) plot_data \u0026lt;- tibble(thing = rbeta(1000, shape1 = 2, shape2 = 5)) %\u0026gt;% mutate(thing = thing * 100) head(plot_data) ## # A tibble: 6 × 1 ## thing ## \u0026lt;dbl\u0026gt; ## 1 10.1 ## 2 34.5 ## 3 55.3 ## 4 2.19 ## 5 38.0 ## 6 39.9 ggplot(plot_data, aes(x = thing)) + geom_histogram(binwidth = 2, color = \u0026quot;white\u0026quot;, boundary = 0)  It worked! Most values are around 20ish, but some go up to 60–80.\nBinomial distribution Often you\u0026rsquo;ll want to generate a column that only has two values: yes/no, treated/untreated, before/after, big/small, red/blue, etc. You\u0026rsquo;ll also likely want to control the proportions (25% treated, 62% blue, etc.). You can do this in two different ways: (1) sample() and (2) rbinom().\nsample() We already saw sample() when we talked about uniform distributions. To generate a binary variable with sample(), just feed it a list of two possible values:\nset.seed(1234) # Choose 5 random T/F values possible_things \u0026lt;- c(TRUE, FALSE) sample(possible_things, 5, replace = TRUE) ## [1] FALSE FALSE FALSE FALSE TRUE  R will choose these values with equal/uniform probability by default, but you can change that in sample() with the prob argument. For instance, pretend you want to simulate an election. According to the latest polls, one candidate has an 80% chance of winning. You want to randomly choose a winner based on that chance. Here\u0026rsquo;s how to do that with sample():\nset.seed(1234) candidates \u0026lt;- c(\u0026quot;Person 1\u0026quot;, \u0026quot;Person 2\u0026quot;) sample(candidates, size = 1, prob = c(0.8, 0.2)) ## [1] \u0026quot;Person 1\u0026quot;  Person 1 wins!\nIt\u0026rsquo;s hard to see the weighted probabilities when you just choose one, so let\u0026rsquo;s pretend there are 1,000 elections:\nset.seed(1234) fake_elections \u0026lt;- tibble(winner = sample(candidates, size = 1000, prob = c(0.8, 0.2), replace = TRUE)) fake_elections %\u0026gt;% count(winner) ## # A tibble: 2 × 2 ## winner n ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 Person 1 792 ## 2 Person 2 208 ggplot(fake_elections, aes(x = winner)) + geom_bar()  Person 1 won 792 of the elections. Neat.\n(This is essentially what election forecasting websites like FiveThirtyEight do! They just do it with way more sophisticated simulations.)\nrbinom() Instead of using sample(), you can use a formal distribution called the binomial distribution. This distribution is often used for things that might have \u0026ldquo;trials\u0026rdquo; or binary outcomes that are like success/failure or yes/no or true/false\nThe binomial distribution takes two parameters:\n size: The number of \u0026ldquo;trials\u0026rdquo;, or times that an event happens prob: The probability of success in each trial  It\u0026rsquo;s easiest to see some examples of this. Let\u0026rsquo;s say you have a program that has a 60% success rate and it is tried on groups of 20 people 5 times. The parameters are thus size = 20 (since there are twenty people per group) and prob = 0.6 (since there is a 60% chance of success):\nset.seed(1234) rbinom(5, size = 20, prob = 0.6) ## [1] 15 11 11 11 10  The results here mean that in group 1, 15/20 (75%) people had success, in group 2, 11/20 (55%) people had success, and so on. Not every group will have exactly 60%, but they\u0026rsquo;re all kind of clustered around that.\nHOWEVER, I don\u0026rsquo;t like using rbinom() like this, since this is all group-based, and when you\u0026rsquo;re generating fake people you generally want to use individuals, or groups of 1. So instead, I assume that size = 1, which means that each \u0026ldquo;group\u0026rdquo; is only one person large. This forces the generated numbers to either be 0 or 1:\nset.seed(1234) rbinom(5, size = 1, prob = 0.6) ## [1] 1 0 0 0 0  Here, only 1 of the 5 people were 1/TRUE/yes, which is hardly close to a 60% chance overall, but that\u0026rsquo;s because we only generated 5 numbers. If we generate lots, we can see the probability of yes emerge:\nset.seed(12345) plot_data \u0026lt;- tibble(thing = rbinom(2000, 1, prob = 0.6)) %\u0026gt;% # Make this a factor since it's basically a yes/no categorical variable mutate(thing = factor(thing)) plot_data %\u0026gt;% count(thing) %\u0026gt;% mutate(proportion = n / sum(n)) ## # A tibble: 2 × 3 ## thing n proportion ## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0 840 0.42 ## 2 1 1160 0.58 ggplot(plot_data, aes(x = thing)) + geom_bar()  58% of the 2,000 fake people here were 1/TRUE/yes, which is close to the goal of 60%. Perfect.\nPoisson distribution One last common distribution that you might find helpful when simulating data is the Poisson distribution (in French, \u0026ldquo;poisson\u0026rdquo; = fish, but here it\u0026rsquo;s not actually named after the animal, but after French mathematician Siméon Denis Poisson).\nA Poisson distribution is special because it generates whole numbers (i.e. nothing like 1.432) that follow a skewed pattern (i.e. more smaller values than larger values). There\u0026rsquo;s all sorts of fancy math behind it that you don\u0026rsquo;t need to worry about so much—all you need to know is that it\u0026rsquo;s good at modeling things called Poisson processes.\nFor instance, let\u0026rsquo;s say you\u0026rsquo;re sitting at the front door of a coffee shop (in pre-COVID days) and you count how many people are in each arriving group. You\u0026rsquo;ll see something like this:\n 1 person 1 person 2 people 1 person 3 people 2 people 1 person  Lots of groups of one, some groups of two, fewer groups of three, and so on. That\u0026rsquo;s a Poisson process: a bunch of independent random events that combine into grouped events.\nThat sounds weird and esoteric (and it is!), but it reflects lots of real world phenomena, and things you\u0026rsquo;ll potentially want to measure in a program. For instance, the number of kids a family has follows a type of Poisson process. Lots of families have 1, some have 2, fewer have 3, even fewer have 4, and so on. The number of cars in traffic, the number of phone calls received by an office, arrival times in a line, and even the outbreak of wars are all examples of Poisson processes.\nYou can generate numbers from a Poisson distribution with the rpois() function in R. This distribution only takes a single parameter:\n lambda ($\\lambda$)  The \\(\\lambda\\) value controls the rate or speed that a Poisson process increases (i.e. jumps from 1 to 2, from 2 to 3, from 3 to 4, etc.). I have absolutely zero mathematical intuition for how it works. The two shape parameters for a Beta distribution at least fit in a fraction and you can wrap your head around that, but the lambda in a Poisson distribution is just a mystery to me. So whenever I use a Poisson distribution for something, I just play with the lambda until the data looks reasonable.\nLet\u0026rsquo;s assume that the number of kids a family has follows a Poisson process. Here\u0026rsquo;s how we can use rpois() to generate that data:\nset.seed(123) # 10 different families rpois(10, lambda = 1) ## [1] 0 2 1 2 3 0 1 2 1 1  Cool. Most families have 0–1 kids; some have 2; one has 3.\nIt\u0026rsquo;s easier to see these patterns with a plot:\nset.seed(1234) plot_data \u0026lt;- tibble(num_kids = rpois(500, lambda = 1)) head(plot_data) ## # A tibble: 6 × 1 ## num_kids ## \u0026lt;int\u0026gt; ## 1 0 ## 2 1 ## 3 1 ## 4 1 ## 5 2 ## 6 1 plot_data %\u0026gt;% group_by(num_kids) %\u0026gt;% summarize(count = n()) %\u0026gt;% mutate(proportion = count / sum(count)) ## # A tibble: 6 × 3 ## num_kids count proportion ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0 180 0.36 ## 2 1 187 0.374 ## 3 2 87 0.174 ## 4 3 32 0.064 ## 5 4 11 0.022 ## 6 5 3 0.006 ggplot(plot_data, aes(x = num_kids)) + geom_bar()  Here 75ish% of families have 0–1 kids (36% + 37.4%), 17% have 2 kids, 6% have 3, 2% have 4, and only 0.6% have 5.\nWe can play with the \\(\\lambda\\) to increase the rate of kids per family:\nset.seed(1234) plot_data \u0026lt;- tibble(num_kids = rpois(500, lambda = 2)) head(plot_data) ## # A tibble: 6 × 1 ## num_kids ## \u0026lt;int\u0026gt; ## 1 0 ## 2 2 ## 3 2 ## 4 2 ## 5 4 ## 6 2 plot_data %\u0026gt;% group_by(num_kids) %\u0026gt;% summarize(count = n()) %\u0026gt;% mutate(proportion = count / sum(count)) ## # A tibble: 8 × 3 ## num_kids count proportion ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0 62 0.124 ## 2 1 135 0.27 ## 3 2 145 0.29 ## 4 3 88 0.176 ## 5 4 38 0.076 ## 6 5 19 0.038 ## 7 6 10 0.02 ## 8 7 3 0.006 ggplot(plot_data, aes(x = num_kids)) + geom_bar()  Now most families have 1–2 kids. Cool.\nRescaling numbers All these different distributions are good at generating general shapes:\n Uniform: a bunch of random numbers with no central gravity Normal: an average ± some variation Beta: different shapes and skews and gravities between 0 and 1 Binomial: yes/no outcomes that follow some probability  The shapes are great, but you also care about the values of these numbers. This can be tricky. As we saw earlier with a normal distribution, sometimes you\u0026rsquo;ll get values that go below zero or above some value you care about. We fixed that with a truncated normal distribution, but not all distributions have truncated versions. Additionally, if you\u0026rsquo;re using a Beta distribution, you\u0026rsquo;re stuck in a 0–1 scale (or 0–10 or 0–100 if you multiply the value by 10 or 100 or whatever).\nWhat if you want a fun skewed Beta shape for a variable like income or some other value that doesn\u0026rsquo;t fit within a 0–1 range? You can rescale any set of numbers after-the-fact using the rescale() function from the scales library and rescale things to whatever range you want.\nFor instance, let\u0026rsquo;s say that income isn\u0026rsquo;t normally distributed, but is right-skewed with a handful of rich people. This might look like a Beta distribution with shape1 = 2 and shape2 = 5:\nggplot() + geom_function(fun = dbeta, args = list(shape1 = 2, shape2 = 5))  If we generate random numbers from this distribution, they\u0026rsquo;ll all be stuck between 0 and 1:\nset.seed(1234) fake_people \u0026lt;- tibble(income = rbeta(1000, shape1 = 2, shape2 = 5)) ggplot(fake_people, aes(x = income)) + geom_histogram(binwidth = 0.1, color = \u0026quot;white\u0026quot;, boundary = 0)  We can take those underling 0–1 values and rescale them to some other range using the rescale() function. We can specify the minimum and maximum values in the to argument. Here we\u0026rsquo;ll scale it up so that 0 = $10,000 and 1 = $100,000. Our rescaled version follows the same skewed Beta distribution shape, but now we\u0026rsquo;re using better values!\nlibrary(scales) fake_people_scaled \u0026lt;- fake_people %\u0026gt;% mutate(income_scaled = rescale(income, to = c(10000, 100000))) head(fake_people_scaled) ## # A tibble: 6 × 2 ## income income_scaled ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0.101 21154. ## 2 0.345 49014. ## 3 0.553 72757. ## 4 0.0219 12176. ## 5 0.380 53036. ## 6 0.399 55162. ggplot(fake_people_scaled, aes(x = income_scaled)) + geom_histogram(binwidth = 5000, color = \u0026quot;white\u0026quot;, boundary = 0)  This works for anything, really. For instance, instead of specifying a mean and standard deviation for a normal distribution and hoping that the generated values don\u0026rsquo;t go too high or too low, you can generate a normal distribution with a mean of 0 and standard deviation of 1 and then rescale it to the range you want:\nset.seed(1234) fake_data \u0026lt;- tibble(age_not_scaled = rnorm(1000, mean = 0, sd = 1)) %\u0026gt;% mutate(age = rescale(age_not_scaled, to = c(18, 65))) head(fake_data) ## # A tibble: 6 × 2 ## age_not_scaled age ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 -1.21 33.6 ## 2 0.277 44.2 ## 3 1.08 49.9 ## 4 -2.35 25.5 ## 5 0.429 45.3 ## 6 0.506 45.8 plot_unscaled \u0026lt;- ggplot(fake_data, aes(x = age_not_scaled)) + geom_histogram(binwidth = 0.5, color = \u0026quot;white\u0026quot;, boundary = 0) plot_scaled \u0026lt;- ggplot(fake_data, aes(x = age)) + geom_histogram(binwidth = 5, color = \u0026quot;white\u0026quot;, boundary = 0) plot_unscaled + plot_scaled  This gives you less control over the center of the distribution (here it happens to be 40 because that\u0026rsquo;s in the middle of 18 and 65), but it gives you more control over the edges of the distribution.\nRescaling things is really helpful when building in effects and interacting columns with other columns, since multiplying variables by different coefficients can make the values go way out of the normal range. You\u0026rsquo;ll see a lot more of that in the synthetic data example.\nSummary Phew. We covered a lot here, and we barely scratched the surface of all the distributions that exist. Here\u0026rsquo;s a helpful summary of the main distributions you should care about:\n   Distribution  Description  Situations  Parameters  Code      Uniform  Numbers between a minimum and maximum; everything equally likely  ID numbers, age  min, max  sample() or runif()    Normal  Numbers bunched up around an average with a surrounding spread; numbers closer to average more likely  Income, education, most types of numbers that have some sort of central tendency  mean, sd  rnorm()    Truncated normal  Normal distribution + constraints on minimum and/or maximum values  Anything with a normal distribution  mean, sd, a (minimum), b (maximum)  truncnorm::rtruncnorm()    Beta  Numbers constrained between 0 and 1  Anything with percents; anything on a 0–1(00) scale; anything, really, if you use rescale() to rescale it  shape1 ($\\alpha$), shape2 ($\\beta$) ($\\frac{\\alpha}{\\alpha + \\beta}$)  rbeta()    Binomial  Binary variables  Treatment/control, yes/no, true/false, 0/1  size, prob  sample(..., prob = 0.5) or rbinom()    Poisson  Whole numbers that represent counts of things  Number of kids, number of cities lived in, arrival times  lambda  rpois()     Example And here\u0026rsquo;s an example dataset of 1,000 fake people and different characteristics. One shortcoming of this fake data is that each of these columns is completely independent—there\u0026rsquo;s no relationship between age and education and family size and income. You can see how to make these columns correlated (and make one cause another!) in the example for synthetic data.\nset.seed(1234) # Set the number of people here once so it's easier to change later n_people \u0026lt;- 1000 example_fake_people \u0026lt;- tibble( id = 1:n_people, opinion = sample(1:5, n_people, replace = TRUE), age = runif(n_people, min = 18, max = 80), income = rnorm(n_people, mean = 50000, sd = 10000), education = rtruncnorm(n_people, mean = 16, sd = 6, a = 8, b = 24), happiness = rbeta(n_people, shape1 = 2, shape2 = 1), treatment = sample(c(TRUE, FALSE), n_people, replace = TRUE, prob = c(0.3, 0.7)), size = rbinom(n_people, size = 1, prob = 0.5), family_size = rpois(n_people, lambda = 1) + 1 # Add one so there are no 0s ) %\u0026gt;% # Adjust some of these columns mutate(opinion = recode(opinion, \u0026quot;1\u0026quot; = \u0026quot;Strongly disagree\u0026quot;, \u0026quot;2\u0026quot; = \u0026quot;Disagree\u0026quot;, \u0026quot;3\u0026quot; = \u0026quot;Neutral\u0026quot;, \u0026quot;4\u0026quot; = \u0026quot;Agree\u0026quot;, \u0026quot;5\u0026quot; = \u0026quot;Strongly agree\u0026quot;)) %\u0026gt;% mutate(size = recode(size, \u0026quot;0\u0026quot; = \u0026quot;Small\u0026quot;, \u0026quot;1\u0026quot; = \u0026quot;Large\u0026quot;)) %\u0026gt;% mutate(happiness = rescale(happiness, to = c(1, 8))) head(example_fake_people) ## # A tibble: 6 × 9 ## id opinion age income education happiness treatment size family_size ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;lgl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 Agree 31.7 43900. 18.3 7.20 TRUE Large 1 ## 2 2 Disagree 52.9 34696. 17.1 4.73 TRUE Large 2 ## 3 3 Strongly agree 45.3 43263. 17.1 7.32 FALSE Large 4 ## 4 4 Agree 34.9 40558. 11.7 4.18 FALSE Small 2 ## 5 5 Strongly disagree 50.3 41392. 13.3 2.61 TRUE Small 2 ## 6 6 Strongly agree 63.6 69917. 11.2 4.36 FALSE Small 2  plot_opinion \u0026lt;- ggplot(example_fake_people, aes(x = opinion)) + geom_bar() + guides(fill = FALSE) + labs(title = \u0026quot;Opinion (uniform with sample())\u0026quot;) ## Warning: `guides(\u0026lt;scale\u0026gt; = FALSE)` is deprecated. Please use `guides(\u0026lt;scale\u0026gt; = \u0026quot;none\u0026quot;)` instead. plot_age \u0026lt;- ggplot(example_fake_people, aes(x = age)) + geom_histogram(binwidth = 5, color = \u0026quot;white\u0026quot;, boundary = 0) + labs(title = \u0026quot;Age (uniform with runif())\u0026quot;) plot_income \u0026lt;- ggplot(example_fake_people, aes(x = income)) + geom_histogram(binwidth = 5000, color = \u0026quot;white\u0026quot;, boundary = 0) + labs(title = \u0026quot;Income (normal)\u0026quot;) plot_education \u0026lt;- ggplot(example_fake_people, aes(x = education)) + geom_histogram(binwidth = 2, color = \u0026quot;white\u0026quot;, boundary = 0) + labs(title = \u0026quot;Education (truncated normal)\u0026quot;) plot_happiness \u0026lt;- ggplot(example_fake_people, aes(x = happiness)) + geom_histogram(binwidth = 1, color = \u0026quot;white\u0026quot;) + scale_x_continuous(breaks = 1:8) + labs(title = \u0026quot;Happiness (Beta, rescaled to 1-8)\u0026quot;) plot_treatment \u0026lt;- ggplot(example_fake_people, aes(x = treatment)) + geom_bar() + labs(title = \u0026quot;Treatment (binary with sample())\u0026quot;) plot_size \u0026lt;- ggplot(example_fake_people, aes(x = size)) + geom_bar() + labs(title = \u0026quot;Size (binary with rbinom())\u0026quot;) plot_family \u0026lt;- ggplot(example_fake_people, aes(x = family_size)) + geom_bar() + scale_x_continuous(breaks = 1:7) + labs(title = \u0026quot;Family size (Poisson)\u0026quot;) (plot_opinion + plot_age) / (plot_income + plot_education)  (plot_happiness + plot_treatment) / (plot_size + plot_family)  ","date":1604102400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604102400,"objectID":"70fead809b2a0b8e7e8fba8c7e04c9e2","permalink":"lbelzile.github.io/math807667a/example/random-numbers/","publishdate":"2020-10-31T00:00:00Z","relpermalink":"lbelzile.github.io/math807667a/example/random-numbers/","section":"example","summary":"Generating fake or simulated data is an incredibly powerful skill, but it takes some practice. Here are a bunch of helpful resources and code examples of how to use different R functions to generate random numbers that follow specific distributions (or probability shapes).","tags":null,"title":"Generating random numbers","type":"docs"},{"authors":null,"categories":null,"content":"Part 1: The basics of R and dplyr For this week\u0026rsquo;s problem set, you need to work through a few of RStudio\u0026rsquo;s introductory primers. You\u0026rsquo;ll do these in your browser and type code and see results there.\nYou\u0026rsquo;ll learn some of the basics of R, as well as some powerful methods for manipulating data with the dplyr package.\nComplete these primers. It seems like there are a lot, but they\u0026rsquo;re short and go fairly quickly (especially as you get the hang of the syntax). Also, I have no way of seeing what you do or what you get wrong or right, and that\u0026rsquo;s totally fine! If you get stuck and want to skip some (or if it gets too easy), go right ahead and skip them!\n The Basics  Visualization Basics Programming Basics   Work with Data  Working with Tibbles Isolating Data with dplyr Deriving Information with dplyr   Visualize Data  Exploratory Data Analysis Bar Charts Histograms Boxplots and Counts Scatterplots Line plots Overplotting and Big Data Customize Your Plots   Tidy Your Data  Reshape Data    Recent versions of tidyr have renamed these core functions: gather() is now pivot_longer() and spread() is now pivot_wider(). The syntax for these pivot_*() functions is slightly different from what it was in gather() and spread(), so you can\u0026rsquo;t just replace the names. Fortunately, both gather() and spread() still work and won\u0026rsquo;t go away for a while, so you can still use them as you learn about reshaping and tidying data. It would be worth learning how the newer pivot_*() functions work, eventually, though (see here for examples).  The content from these primers comes from the (free and online!) book R for Data Science by Garrett Grolemund and Hadley Wickham. I highly recommend the book as a reference and for continuing to learn and use R in the future.\nPart 2: Getting familiar with RStudio The RStudio primers you just worked through are a great introduction to writing and running R code, but you typically won\u0026rsquo;t type code in a browser when you work with R. Instead, you\u0026rsquo;ll use a nicer programming environment like RStudio, which lets you type and save code in scripts, run code from those scripts, and see the output of that code, all in the same program.\nTo get familiar with RStudio, watch this video (it\u0026rsquo;s from PMAP 8921, but the content still applies here):\n  Part 3: RStudio Projects One of the most powerful and useful aspects of RStudio is its ability to manage projects.\nWhen you first open R, it is \u0026ldquo;pointed\u0026rdquo; at some folder on your computer, and anything you do will be relative to that folder. The technical term for this is a \u0026ldquo;working directory.\u0026rdquo;\nWhen you first open RStudio, look in the area right at the top of the Console pane to see your current working directory. Most likely you\u0026rsquo;ll see something cryptic: ~/\nThat tilde sign (~) is a shortcut that stands for your user directory. On Windows this is C:\\Users\\your_user_name\\; on macOS this is /Users/your_user_name/. With the working directory set to ~/, R is \u0026ldquo;pointed\u0026rdquo; at that folder, and anything you save will end up in that folder, and R will expect any data that you load to be there too.\nIt\u0026rsquo;s always best to point R at some other directory. If you don\u0026rsquo;t use RStudio, you need to manually set the working directory to where you want it with setwd(), and many R scripts in the wild include something like setwd(\u0026quot;C:\\\\Users\\\\bill\\\\Desktop\\\\Important research project\u0026quot;) at the beginning to change the directory. THIS IS BAD THOUGH (see here for an explanation). If you ever move that directory somewhere else, or run the script on a different computer, or share the project with someone, the path will be wrong and nothing will run and you will be sad.\nThe best way to deal with working directories with RStudio is to use RStudio Projects. These are special files that RStudio creates for you that end in a .Rproj extension. When you open one of these special files, a new RStudio instance will open up and be pointed at the correct directory automatically. If you move the folder later or open it on a different computer, it will work just fine and you will not be sad.\nRead this super short chapter on RStudio projects to learn how to create and use them\nIn general, you can create a new project by going to File \u0026gt; New Project \u0026gt; New Directory \u0026gt; Empty Project, which will create a new folder on your computer that is empty except for a single .Rproj file. Double click on that file to open an RStudio instance that is pointed at the correct folder.\nPart 4: Getting familiar with R Markdown To ensure that the analysis and graphics you make are reproducible, you\u0026rsquo;ll do the majority of your work in this class using R Markdown files.\nDo the following things:\n Watch this video:      Skim through the content at these pages:\n Using Markdown Using R Markdown How it Works Code Chunks Inline Code Markdown Basics (The R Markdown Reference Guide is super useful here.) Output Formats    Watch this video (again, it\u0026rsquo;s from PMAP 8921, but the content works for this class):\n    ","date":1598227200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598227200,"objectID":"eb73959ad5db65774d90c06c8dbcb55f","permalink":"lbelzile.github.io/math807667a/example/rstudio-tidyverse/","publishdate":"2020-08-24T00:00:00Z","relpermalink":"lbelzile.github.io/math807667a/example/rstudio-tidyverse/","section":"example","summary":"Part 1: The basics of R and dplyr For this week\u0026rsquo;s problem set, you need to work through a few of RStudio\u0026rsquo;s introductory primers. You\u0026rsquo;ll do these in your browser and type code and see results there.","tags":null,"title":"Welcome to R, RStudio, and the tidyverse!","type":"docs"},{"authors":null,"categories":null,"content":"  Here’s your roadmap for the semester!\n Content (): This page contains the readings, slides, and recorded lectures for the topic. Read and watch these first. Example (): This page contains fully annotated R code and other supplementary information that you can use as a reference for your assignments and project. This is only a reference page—you don’t have to necessarily do anything here. Some sections also contain videos of me live coding the examples so you can see what it looks like to work with R in real time. This page will be very helpful as you work on your assignments. Assignment (): This page contains the instructions for each assignment. Assignments are due by 11:55 PM on the day they’re listed.  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3e223d7ba58b0122b42458e4cf52e04c","permalink":"lbelzile.github.io/math807667a/schedule/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"lbelzile.github.io/math807667a/schedule/","section":"","summary":"Here’s your roadmap for the semester!\n Content (): This page contains the readings, slides, and recorded lectures for the topic. Read and watch these first. Example (): This page contains fully annotated R code and other supplementary information that you can use as a reference for your assignments and project.","tags":null,"title":"Schedule","type":"page"}]