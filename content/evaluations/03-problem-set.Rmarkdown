---
title: "Problem set 3"
linktitle: "Problem set 3"
date: "2021-09-22"
due_date: "2021-10-01"
due_time: "11:55 PM"
menu:
  evaluations:
    parent: Problem sets
    weight: 3
type: docs
toc: true
bibliography: "../../static/bib/references.bib"
csl: "../../static/bib/apa.csl"
---

# Task 1 - Model assumptions

The purpose of this exercise is to explore what happens when the model assumptions of the one-way analysis of variance go wrong using simulations. The interpretation of most statements we encounter (95% of the time) are to be understood in terms of repeated sampling. In practice, this means we can create fake data in **R** and look at the _what if_. This is useful to build your intuition about what could go wrong, keeping in mind that you will have a single realization at end.

The workflow, coded for you `r emo::ji("grinning face")`, is the following:

The $F$-distribution of the one-way analysis of variance is derived under the assumption that the variance are equal.

1. Null hypothesis is true, means are equal:
  - we simulate observations from two sub-populations having different variance
2. An alternative hypothesis is true, population means are unequal
  - we simulate observations from sub-populations having different means and different variances.

```{r }

simulation_study <- 
  function(ngroup = 3L,
           nsamp = 20L,
           var.equal = TRUE,
           ){
  
}
```{r pvaluedist, eval = TRUE, echo = TRUE, cache = TRUE}
ng <- 4L #number of groups
nsamp <- 20L #number of observations per group
ntot <- ng*nsamp #number of observations (total)
nrep <- 10000L #number of replications
# Create container to store results
pval <- vector(mode = "numeric", length = nrep)
fstat <- vector(mode = "numeric", length = nrep)
# For-loop: repeat these instructions nrep times
for(i in 1:nrep){
  sample <-
    tibble(response = rnorm(n = ntot), # sample No(0,1) variables
           group = factor(rep(1:ng, each = nsamp)))
  # normal data (rnorm), by default with
  # mean zero, std. dev 1
  ## Compute the ANOVA F-statistic
  ftest <- anova(lm(response ~ group, data = sample))
  # Make table to extract p-values more easily
  ftest_tidy <- broom::tidy(ftest)
  # Store the F-statistic and the p-value in the container
  fstat[i] <- ftest_tidy$statistic[1]
  pval[i] <- ftest_tidy$p.value[1]
}
```

If the null distribution is well calibrated (model assumptions hold and data arise from the null model), then the distribution of the _p_-values is uniform: every number between zero and one is equally likely, so the probability that the result is less than the level $\alpha$ is on average $\alpha$. We can create a histogram of the 10K _p_-values and compare them to the uniform distribution.

```{r plotpval, echo = TRUE, eval = TRUE, cache = TRUE}
ggplot(data = tibble("pvalue" = pval),
       aes(x = pvalue)) +
  # bin into 20 compartments,
  # specifying boundaries to avoid artifacts
  geom_histogram(breaks = seq(0, 1, by = 0.05),
                 aes(y = ..density..),
                 alpha = 0.2) +
  stat_function(fun = dunif, #uniform distribution
                col = "blue") +
  labs(x = "p-value",
       y = "density",
       subtitle = "Distribution of p-values under the null hypothesis",
       caption = "based on 10 000 simulations.") +
   scale_x_continuous(expand = c(0, 0),
                      limits = c(0, 1),
                      breaks = c(0,0.5,1)) +
   geom_vline(xintercept = alpha, type = 2)
# Compute the proportion of p-values from the
# 10K replications that are less than alpha
mean(pval < alpha) #pval < alpha is logical, 
# with FALSE=0 and TRUE=1 (pval < alpha)
```

We can look directly at the possible values of the test statistic instead, as in Figure \@ref(fig:pvaluedist), which provides the same conclusions albeit on a different scale. The parameters of the (reference) $F$ distribution (large sample approximation) are called degrees of freedom: they are respectively the number of additional parameters relative to the null hypothesis, $K-1$ for $K$ groups, and the the number of observations minus the number of mean parameters under the alternative, $n-K$. We reject for any instance where the null statistic exceeds the ($1-\alpha$) quantile of the null distribution, represented by the simulation. The proportion of statistics that exceed the cutoff is `r mean(fstat > cutoff)` which leads to the same conclusion as using the _p_-values: this is unsurprising given that the _p_-values are derived from this approximation in the first place, so there is a one-to-one correspondence between the two.


```{r pvaluedist, eval = TRUE, echo = TRUE, cache = TRUE}
cutoff <- qf(0.95, ng - 1, ntot - ng) 
#cutoff: reject if larger than this value
ggplot(data = tibble("stat" = fstat),
       aes(x = stat)) +
  geom_histogram(aes(y = ..density..), #simulated null distribution of F
                 bins = 100L) +
  stat_function(fun = df, #theoretical (large sample) approximation
                args = list(df1 = ng - 1,
                            df2 = ng*nsamp - ng),
                n = 1000) +
  labs(x = "F statistic")
mean(fstat > cutoff)
```