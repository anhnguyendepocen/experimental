---
title: "Problem set 4"
linktitle: "Problem set 4"
date: "2021-10-02"
due_date: "2021-10-12"
due_time: "11:55 PM"
menu:
  evaluations:
    parent: Problem sets
    weight: 4
type: docs
toc: true
bibliography: "../../static/bib/references.bib"
csl: "../../static/bib/apa.csl"
---


## Task 1 - Selective reporting and biased effect sizes

Check out [*Understanding $p$-values Through Simulations*](https://rpsychologist.com/pvalue/) by Kristopher Magnusson. 

The app aims to show why estimated effect sizes are biased upward by postulating that only studies achieving statistical significance get reported. 

If there is no difference between groups (Cohen's $d$ is zero), the average reported effect size $\widehat{d}$ for the subset of studies that show 'statistical significance' at the 5% level is very large.

More interesting is the case where the null is false and the true Cohen's $d$ is smallish: shift the curve by increments of 0.1

1. Play with the app by modifying (in turn) the following parameters. Does it alleviate part of the effect size bias (the last value in the table)?
	a. increase the sample size per group
	b. increase the true effect size by shifting the curve to the right. 
	c. The $p$-hack tool is used to add an observation to each sample for which the value was 'not statistically significant' initially until it becomes significant. Note that studies for which initial estimates showed significance are not affected. This point is related to a statistical fallacy: if you peak at the data as you collect observations and make the analysis sequentially until you achieve statistical significance, estimated are biased unless you account explicitly for this stopping rule through conditioning.

How might this selective reporting affect calculations such as the one of Task 2? 


## Task 2 - Power calculation for a replication study

@Johnson:2014 performs a replication study of Schnall, Benton, and Harvey (2008) who conjectured that physical cleanliness reduces the severity of moral judgments. 
The following excerpt from the paper explain how sample size for the replication were calculated.

> In Experiment 2, the critical test of the cleanliness manipulation on ratings of morality was significant, $F(1, 41) = 7.81$, $p=0.01$, $d=0.87$, $N=44$. Assuming $\alpha=0.05$, the achieved power in this experiment was $0.80$. Our proposed research will attempt to replicate this experiment with a level of power = $0.99$. This will require a minimum of 100 participants (assuming equal sized groups with $d=0.87$) so we will collect data from 115 participants to ensure a properly powered sample in case of errors.

1. Read the abstract of the paper. Are your surprised by the findings?
2. In your own words, explain why it is incorrect to talk about 'achieved power' (aka observed power). *Hint: think back about Task 1*
3. Validate the calculation provided in the quote using `pwr::pwr.t.test`
4. Calculate the sample size necessary to obtain power of $0.80$ and $0.90$ in case the true effect size was $d=0.01$. Discuss the feasibility of detecting such an effect.



## References

