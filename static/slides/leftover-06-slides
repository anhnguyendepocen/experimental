

class: title title-4

# Example from the OSC psychology replication

> People can be influenced by the prior consideration of a numerical anchor when forming numerical judgments. [...]  The anchor provides an initial starting point from which estimates are adjusted, and a large body of research demonstrates that adjustment is usually insufficient, leading estimates to be biased towards the initial anchor.

.small[
[Replication of Study 4a of Janiszewski & Uy (2008, Psychological Science) by J. Chandler](https://osf.io/aaudl/)
]

???

People can be influenced by the prior consideration of a numerical anchor when forming numerical judgments. The anchor provides an initial starting point from which estimates are adjusted, and a large body of research demonstrates that adjustment is usually insufficient, leading estimates to be biased towards the initial anchor. Extending this work, Janiszewski and Uy (2008) conceptualized people's attempt to adjust following presentation of an anchor as movement along a subjective representation scale by a certain number of units. Precise numbers (e.g. 9.99) imply a finer-resolution scale than round numbers (e.g. 10). Consequently, adjustment along a subjectively finer resolution scale will result in less objective adjustment than adjustment by the same number of units along a subjectively coarse resolution scale. 

In three experimental studies the authors demonstrate this predicted basic effect and rule out various alternative explanations. Two additional studies (4a and b) found that this effect was especially strong when people were explicitly given more motivation to adjust their estimates (e.g., by implying that the initial anchor substantially overestimated the price). 


class: title title-5
# Breaking down the variability

For a **balanced** design, we can decompose the variability around sample means:


$$\begin{align*}\mathsf{SS}_{\text{total}} &= \mathsf{SS}_{\text{model}} + \mathsf{SS}_{\mathsf{res}}
\\&= \mathsf{SS}_{A} + \mathsf{SS}_{B} + \mathsf{SS}_{AB}+ \mathsf{SS}_{\mathsf{res}}\end{align*}$$

???

This stems from the orthogonality between the factors.

---

---
class: title title-5
# Example: Fiber strength

We consider a $5 \times 3$ factorial design (no interaction term).

Consider five levels of application of potash

- $T_1=36$, $T_2=54$, $T_3=72$, $T_4=108$ and $T_5=144$ lb K<sub>2</sub>O per acre, applied to a cotton crop. 

The response is a measure of single-fiber strength, an average of a number of tests on the cotton from each plot.

There were three blocks each containing five plots.

---
class: title title-5
# Data on fiber strength

.pull-left[

```{r kableData, echo = FALSE}
CoxCochran <- data.frame(
  block = factor(rep(paste("block", 1:3), each = 5L)),
  treatment = factor(rep(paste0("T", 1:5), length.out = 15L)),
  measurements = c(7.62, 8.14, 7.76, 7.17, 7.46, 8.00, 8.15,
                   7.73, 7.57, 7.68, 7.93, 7.87, 7.74, 7.80, 7.21)
)
knitr::kable(digits = c(0, rep(2, 5L)), 
               caption = "Reordered data",
  x = pivot_wider(data = CoxCochran,
              values_from = measurements, 
              names_from = treatment)
)

twowayANOVA <- lm(measurements ~ treatment + block, 
                  data = CoxCochran)
CoxCochran$residuals <- resid(twowayANOVA)
```

]

.pull-right[

```{r residualsTable, echo = FALSE}
tab_resid <- CoxCochran %>% 
  select(!measurements) %>%
  pivot_wider(values_from = residuals, 
              names_from = treatment)
knitr::kable(digits = c(0, rep(2, 5L)), 
               caption = "Residuals",
  x = tab_resid
)
```



]



???
From Cox (1958) Planning of experiments

Sources of variability:
 - cultivation and harvesting of the crop
 - selection of fibers for the test
 - strength testing
 
---
class: title title-5
# Analysis of variance table

```{r ANOVAtabCox, eval = TRUE, echo = FALSE}
knitr::kable(car::Anova(twowayANOVA, type = "II"),
             digits = c(3,0,3,3),
             col.names = c("sum of squares",
                           "df","F", "p-value"))
```

---
class: title title-5
# Some pending questions

- Intuition behind degrees of freedom for the residuals?
- No interaction term (why?)
- Why blocking?

--

| $A$ \\ $B$ | $b_1$ | $b_2$ | $b_3$ | $b_4$ | $b_5$ | sum |
|---|:---:|:---:|:---:|:---:|:--:|:--:|
| $a_1$ | $AB_{11}$ | $AB_{12}$ | $AB_{13}$ |  $AB_{14}$ | $\mathsf{X}$ | $A_1$ |
| $a_2$ | $AB_{21}$ | $AB_{22}$ | $AB_{23}$ | $AB_{24}$ | $\mathsf{X}$ | $A_2$ |
| $a_3$ | $\mathsf{X}$ | $\mathsf{X}$ | $\mathsf{X}$ | $\mathsf{X}$ | $\mathsf{X}$ | $\mathsf{X}$ |
| **sum** | $B_1$ | $B_2$ | $B_3$ | $B_4$ | $\mathsf{X}$ | total |

.tiny[ 
Terms with $\mathsf{X}$ are fully determined by row/column/total averages

]

---


---
class: title title-5
# Example from Keppel and Wichern (table)

Consider errors by monkeys under three drug conditions ( $A$ ) and two degrees of food deprivation ( $B$ )


.box-inv-5.sp-after-half[
Data for the $3 \times 2$ factorial design
]


|  $A$ \\  $B$  | 1h deprivation | 24h deprivation|
|----|:-----:|:-----:|
| **Control** | 1, 4, 0, 7 | 15, 6, 10, 13 |
| **Drug 1** | 13, 5, 7, 15 | 6, 18, 9, 15 |
| **Drug 2** | 9, 16, 18, 13 | 14, 7, 6, 13 |

.box-inv-5[**R** Demonstration]

---

class: title title-6
# Noncentrality parameters

Consider a balanced design with $n=a \times b \times k$ observations 

For the mean squared errors, the expected values are

$$\mathsf{E}(\mathsf{MS}_A) = \sigma^2 + \frac{bn}{a-1} \sum_{i=1}^a \alpha_i^2$$
$$\mathsf{E}(\mathsf{MS}_{AB}) = \sigma^2 + \frac{n}{(a-1)(b-1)} \sum_{j=1}^b\sum_{i=1}^a (\alpha\beta)_{ij}^2$$


Under the null hypothesis of no mean effect / interaction, these are thus unbiased estimators of the error variance.

???

$k$ replications for each level
Under the null hypothesis of no mean effect / interaction, these are thus unbiased estimators of the error variance

---
