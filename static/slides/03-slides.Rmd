---
title: "One way ANOVA"
author: "Léo Belzile"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    lib_dir: "libs"
    chakra: "libs/remark-latest.min.js"
    css: ["default", "css/ath-slides.css", "css/ath-inferno-fonts.css", "css/animate.css"]
    seal: false
    anchor_sections: false
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
      navigation:
        scroll: false
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, 
                      fig.retina = 3, fig.align = "center")
```
```{r packages-data, echo = FALSE, include=FALSE}
library(tidyverse)
library(patchwork)
```
```{r xaringanExtra, echo=FALSE}
xaringanExtra::use_xaringan_extra(c("tile_view","freezeframe","panelset","clipboard"))
xaringanExtra::use_broadcast()
```

class: center middle main-title section-title-1

# One way ANOVA

.class-info[

**Session 3**

.light[MATH 80667A: Experimental Design and Statistical Methods <br>for Quantitative Research in Management <br>
HEC Montréal
]

]

---

name: outline
class: title title-inv-5

# Outline
--

.box-2.medium.sp-after-half[Recap]

--

.box-8.medium.sp-after-half[F tests]

--

.box-4.medium.sp-after-half[Parametrizations and interpretation]

--

.box-3.medium.sp-after-half[Planned comparisons and *post-hoc* tests]


---

layout: false
name: recap
class: center middle section-title section-title-2 animated fadeIn

# Refresher on hypothesis tests

---

class: title title-2

# General recipe of hypothesis testing

.pull-left[

.box-2.sp-after-half[
(1) Define variables
]

.box-2.sp-after-half[
(2) Write down hypotheses
]

.box-2.sp-after-half[
(3) Choose/compute a test statistic
]

.box-2.sp-after-half[
(4) Benchmark
]

]

.pull-right[

.box-2.sp-after-half[
(5) Compute the _p_-value
]
.box-inv-2.sp-after-half[
(6) Conclude (reject/fail to reject)
]
.box-2.sp-after-half[
(7) Report findings
]

]

---

class: title title-2

# Level

.box-2.sp-after-half[
Level = probability of judicial error 


Analyst fixes **level** $\alpha$
**before** the experiment.
]

.box-inv-2.sp-after-half[

Choose $\alpha$ as small as possible (don't condemn the innocent!)

(typical value is 5%)
]

.box-2.sp-after-half[
Reject $\mathscr{H}_0$ if p-value less than $\alpha$
]


???

Question: why can't we fix $\alpha=0$?



---

class: title title-2
# Errors


```{r nullvsalternative, eval = TRUE, echo = FALSE, cache = TRUE, fig.height=7, fig.width = 11, fig.align ='center'}
g1 <- ggplot() + 
  stat_function(xlim = c(0, 8), 
                n = 1001,
                fun=df, args = list(df1 = 4, df2 = 80, ncp = 0)) + 
  stat_function(geom = "area", fill = "blue", alpha = 0.2,
                xlim = c(qf(0.95, 4,80),8), 
                n = 1001,
                fun=df, args = list(df1 = 4, df2 = 80, ncp = 0)) + 
  labs(x = "statistic", 
       y = "density", 
       subtitle = "null is true") +  
  scale_y_continuous(expand = c(0, 0)) + 
  theme_classic()
g2 <- ggplot() + 
  stat_function(xlim = c(0, 8), 
                n = 1001,
                fun=df, args = list(df1 = 4, df2 = 80, ncp = 3)) + 
  stat_function(geom = "area", fill = "blue", alpha = 0.2,
                xlim = c(0, qf(0.95, 4,80)), 
                n = 1001,
                fun=df, args = list(df1 = 4, df2 = 80, ncp = 3)) +  
  labs(x = "statistic", 
       y = "density", subtitle = "alternative (null is false)") + scale_y_continuous(expand = c(0, 0)) + 
  theme_classic()
  
  
g1 + g2 
```

???

- Talk about cutoff values
- Discuss the tradeoff between type I and type II errors

---

class: title title-2

# Confidence intervals

Test statistics are standardized, 
- Good for comparisons with benchmark
- typically meaningless (standardized = unitless quantities)

Two options for reporting: 

- $p$-value: probability of more extreme outcome if no mean difference
- confidence interval: set of all values for which we fail to reject the null hypothesis at level $\alpha$ for the given sample

---
class: title title-2

# Interpretation of confidence intervals


.box-7.sp-after-half[ confidence interval = [lower, upper] units]

.small[
Interpretation: under the null, if we repeat the experiments 95%, of intervals will contain the true value (if model correctly calibrated)]


.box-7.sp-after-half[Analogy: coin toss]

```{r out.width = '50%', out.height = '40%', fig.alt = "100 confidence intervals", eval = TRUE, echo = FALSE}
knitr::include_graphics("img/03/confint.png")
```

---

layout: false
name: f-test
class: center middle section-title section-title-8 animated fadeIn

# One way analysis of variance

---

class: title title-8

# F-test for one way ANOVA

.box-inv-8.medium[Global null hypothesis]

No difference between treatments

- $\mathscr{H}_0$ (null): all of the $K$ treatment groups have the same average $\mu$
- $\mathscr{H}_a$ (alternative): at least two treatments have different averages

???

- The null hypothesis can be viewed as a special case from a bigger class of possibilities
- it always corresponds to some restrictions from the alternative class

---

class: title title-8

# _F_-test statistic

.box-8.medium[Omnibus test]

With $K$ groups and $n$ observations, the statistic is

\begin{align*}
F =  \frac{\text{between sum of squares}/(K-1)}{\text{within sum of squares}/(n-K)}
\end{align*}


The null distribution (benchmark) is $F(K-1, n-K)$.

---

class: title title-8

# Why does it work?

.small[
Denote
- $y_{ik}$ is observation $i$ of group $k$
- $\widehat{\mu}_1, \ldots, \widehat{\mu}_K$ the sample average of groups $1, \ldots, K$
- $\widehat{\mu}$ is overall sample mean

]

.box-8[Decomposing variability into bits]

\begin{align*}
\underset{\text{total sum of squares}}{\sum_{i}\sum_{k} (y_{ik} - \widehat{\mu})^2} &= \underset{\text{within sum of squares}}{\sum_i \sum_k (y_{ik} - \widehat{\mu}_k)^2} +  \underset{\text{between sum of squares}}{\sum_k n_i (\widehat{\mu}_k - \widehat{\mu})^2}.
\end{align*}

.pull-left-3[
.box-inv-8[null model]
]
.pull-middle-3[
.box-inv-8[alternative model]
]
.pull-right-3[
.box-inv-8[added variability]
]

---


class: title title-8

# Degrees of freedom

The parameters of the null distribution are called **degrees of freedom**

- $K-1$ is the number of constraints imposed by the null
- $n-K$ is the number of observations minus number of mean parameters estimated under alternative

Idea of _F_-statistic: under the null, both numerator and denominator are variance estimators.

- but the numerator is more variable...
- the $F$ ratio should be approximately one on average

---

layout: false
name: parametrization
class: center middle section-title section-title-4 animated fadeIn

# Parametrizations and interpretation

---

class: title title-4

# Parametrization 1: sample averages

Most natural parametrization, not useful for test

- Sample sizes in each group: $n_1, \ldots, n_K$, are known.

- sample average of each treatment group: $\widehat{\mu}_1, \ldots, \widehat{\mu}_K$.

.box-inv-4.sp-after-half[
$K$ means  = $K$ parameters
]

Overall mean is 
\begin{align*}
n \widehat{\mu} = n_1 \widehat{\mu}_1 + \cdots + n_K \widehat{\mu}_K
\end{align*}


---
class: title title-4

# Parametrization 2: contrasts

In terms of differences, relative to a baseline category $j$



- Intercept = sample mean $\widehat{\mu}_j$
- Coefficient for group $k \neq j$: $\widehat{\mu}_k - \widehat{\mu}_j$
    - difference between averages of group $k$ and baseline

In **R**, the baseline is the smallest alphanumerical value.

.box-inv-4[
```{r echo = TRUE, eval = FALSE}
lm(response ~ group)
```
]

---
class: title title-4

# Parametrization 3: sum-to-zero

In terms of differences, relative to average of $\widehat{\mu}_1, \ldots, \widehat{\mu}_K$

- Intercept = $(\widehat{\mu}_1 + \cdots + \widehat{\mu}_K)/K$
- Coefficient for group $k$: $\widehat{\mu}_k$ minus intercept

In **R**, the last factor level is dropped by default.

.box-inv-4[
```{r echo = TRUE, eval = FALSE}
lm(response ~ group, contrasts = contr.sum(group))
```
]

Warning: Intercept $\neq \widehat{\mu}$ unless the sample is balanced

---
class: title title-4

# Comparison for the arithmetic example


<table>
 <thead>
  <tr>
   <th style="text-align:left;"> group </th>
   <th style="text-align:right;"> mean </th>
   <th style="text-align:right;"> contrasts </th>
   <th style="text-align:right;"> sum-to-zero </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:left;"> intercept </td>
   <td style="text-align:right;">  </td>
   <td style="text-align:right;"> 19.66 </td>
   <td style="text-align:right;"> 21.00 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> control 1 </td>
   <td style="text-align:right;"> 19.66 </td>
   <td style="text-align:right;">  </td>
   <td style="text-align:right;"> -1.33 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> control 2 </td>
   <td style="text-align:right;"> 18.33 </td>
   <td style="text-align:right;"> -1.33 </td>
   <td style="text-align:right;"> -2.66 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> praised </td>
   <td style="text-align:right;"> 27.44 </td>
   <td style="text-align:right;"> 7.77 </td>
   <td style="text-align:right;"> 6.44 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> reproved </td>
   <td style="text-align:right;"> 23.44 </td>
   <td style="text-align:right;"> 3.77 </td>
   <td style="text-align:right;"> 2.44 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> ignored </td>
   <td style="text-align:right;"> 16.11 </td>
   <td style="text-align:right;"> -3.55 </td>
   <td style="text-align:right;">  </td>
  </tr>
</tbody>
</table>

---
layout: false
name: planned-comparisons
class: center middle section-title section-title-3 animated fadeIn

# Planned comparisons and post-hoc tests

---

class: title title-3

# Planned comparisons

Oftentimes, we are not interested in the global null hypothesis.

- Can formulate planned comparisons *at registration time* for effects of interest

.box-inv-3.large[What is the scientific question of interest?]

---

class: title title-3

# Arithmetic example

.box-3.medium[Setup]
.pull-left-3[
.box-inv-3[
group 1 
]
.center[
(control)
]
]
.pull-middle-3[
.box-inv-3.sp-after-half[
group 2 
]
.center[
(control)
]
]

.pull-right-3[
.box-inv-3.sp-after-half[
group 3
]
.center[
(praise, reprove, ignore)
]
]

--

.box-3.medium[Hypothesis of interest]

- $\mathscr{H}_{01}$: $\mu_{\text{praise}} = \mu_{\text{reproved}}$ (attention)
- $\mathscr{H}_{02}$: $\frac{1}{2}(\mu_{\text{control}_1}+\mu_{\text{control}_2}) = \mu_{\text{praised}}$ (encouragement)

???

This is post-hoc, but supposed we had particular interest in the following hypothesis (for illustration purposes)

---

class: title title-3

# Contrasts

With placeholders for each group, write
$\mathscr{H}_{01}: \mu_{\text{praised}} = \mu_{\text{reproved}}$ as 
.small.center[

$0\cdot \mu_{\text{control}_1}$ + $0\cdot \mu_{\text{control}_2}$ + $1 \cdot \mu_{\text{praised}}$ - $1\cdot \mu_{\text{reproved}}$ + $0 \cdot \mu_{\text{ignored}}$

]

The sum of the coefficients, $(0, 0, 1, -1, 0)$, is zero.

.box-3[Contrast = sum-to-zero constraint]

--

Similarly, for $\mathscr{H}_{02}: \frac{1}{2}(\mu_{\text{control}_1}+\mu_{\text{control}_2}) = \mu_{\text{praise}}$
.small.center[

$\frac{1}{2} \cdot \mu_{\text{control}_1}$ + $\frac{1}{2}\cdot \mu_{\text{control}_2}$ - $1 \cdot \mu_{\text{praised}}$ + $0\cdot \mu_{\text{reproved}}$ + $0 \cdot \mu_{\text{ignored}}$

]

The contrast vector $\left(\frac{1}{2}, \frac{1}{2}, -1, 0, 0\right)$ sums to zero.

Equivalent formulation is obtained by picking $(1,1,-2,0,0)$

---
class: title title-3

# Contrasts in **R**

```{r eval = FALSE, echo = TRUE}
library(emmeans)
linmod <- lm(score ~ group, data = arithmetic)
linmod_emm <- emmeans(linmod, specs = 'group')
contrast_specif <- list(
  controlvspraised = c(0.5, 0.5, -1, 0, 0),
  praisedvsreproved = c(0, 0, 1, -1, 0)
)
contrasts_res <- 
  contrast(object = linmod_emm, 
                    method = contrast_specif)
# Obtain confidence intervals instead of p-values
confint(contrasts_res)
```

---


class: title title-3

# *Post-hoc* tests

Maybe there is some difference between groups?

Unplanned comparisons: go fishing...

.box-inv-3[
Comparing all pairwise differences = $\binom{K}{2}$ tests]

With $K=5$ groups, we get 10 pairwise comparisons.

```{r eval = FALSE}
emmeans(modlin, pairwise ~ group)
```

If there were no differences between the groups, how many do we expect to find significant by chance with $\alpha = 0.1$?

---

class: title title-3

# Pairwise differences and _t_-tests

.box-inv-3.left[Technical aside]

The pairwise differences (_p_-values) and confidence intervals for groups $j$ and $k$ are based on the _t_-statistic:

\begin{align*}
t = \frac{\text{estimated} - \text{postulated difference}}{\text{uncertainty}}= \frac{(\widehat{\mu}_j - \widehat{\mu}_k) - (\mu_j - \mu_k)}{\mathsf{se}(\widehat{\mu}_j - \widehat{\mu}_k)}
\end{align*}
which has a null $\mathsf{St}(n-k)$ distribution.

The standard error $\mathsf{se}(\widehat{\mu}_j - \widehat{\mu}_k)$ uses the pooled variance estimate (based on all groups).

---
class: title title-3
# Null distribution


The blue area defines the set of values for which we fail to reject null $\mathscr{H}_0$.

All values of $t$ falling in the red aread lead to rejection at level $5$%.

```{r tcurve, eval = TRUE, echo = FALSE, retina = 3, out.width = '60%', fig.asp = 0.618}
ggplot() +
  coord_cartesian(xlim = c(-5,5), 
                  ylim = c(0, 0.5), 
                  expand = FALSE) +
  stat_function(fun = dt, 
                args = list(df = 40), 
                xlim = c(qt(0.975, df = 40),5),
                geom = "area", 
                fill = "red", 
                alpha = 0.2) +
    stat_function(fun = dt, 
                args = list(df = 40), 
                xlim = c(-5, qt(0.025, df = 40)),
                geom = "area", 
                fill = "red", 
                alpha = 0.2) +
    stat_function(fun = dt, 
                args = list(df = 40), 
                xlim = c(qt(0.025, df = 40), qt(0.975, df = 40)),
                geom = "area", 
                fill = "blue", 
                alpha = 0.2) + 
    stat_function(fun = dt, 
    xlim = c(-5,5),
                args = list(df = 40), n = 1000) + 
       theme_classic() + labs(y = "density", x = "")
```


---
class: title title-3

# _t_-tests

If we postulate $\Delta_{jk} = \mu_j - \mu_k = 0$, the test statistic becomes

\begin{align*}
t = \frac{(\widehat{\Delta}_{jk}) - 0}{\mathsf{se}(\widehat{\Delta}_{jk})}
\end{align*}

The $p$-value is $p = 1- \Pr(-|t| \leq \mathsf{St}_{n-k} \leq |t|)$.


The larger the values of $t$ (positive or negative), the more evidence against the null hypothesis.

---

class: title title-3
# Critical values

For a test at level $\alpha$ (two-sided), fail to reject all values of the test statistic $t$ that are in interval

$$\mathfrak{t}_{n-k}(\alpha/2) \leq t \leq \mathfrak{t}_{n-k}(1-\alpha/2)$$

Because of symmetry around zero, $\mathfrak{t}_{n-k}(1-\alpha/2) = -\mathfrak{t}_{n-k}(\alpha/2)$.

- We call $\mathfrak{t}_{n-k}(1-\alpha/2)$ a **critical value**. 
- in **R**, `qt(1-alpha/2, df = n - k)` where `n` is the number of observations and `k` the number of groups

---
class: title title-3
# Confidence interval 

.small[

Let $\Delta_{jk}=\mu_j - \mu_k$ denote the population difference, $\widehat{\Delta}_{jk}$ the estimated difference (difference in sample averages) and $\mathsf{se}(\widehat{\Delta}_{jk})$ the estimated standard error.

The region for which we fail to reject the null is 
\begin{align*}
\mathfrak{t}_{n-k}(\alpha/2) \leq  \frac{\widehat{\Delta}_{jk} - \Delta_{jk}}{\mathsf{se}(\widehat{\Delta}_{jk})} \leq \mathfrak{t}_{n-k}(1-\alpha/2)
\end{align*}
which rearranged gives the $(1-\alpha)$ confidence interval for the (unknown) difference $\Delta_{jk}$.

\begin{align*}
\widehat{\Delta}_{jk} + \mathsf{se}(\widehat{\Delta}_{jk})\mathfrak{t}_{n-k}(\alpha/2) \leq \Delta_{jk} \leq \widehat{\Delta}_{jk} + \mathsf{se}(\widehat{\Delta}_{jk})\mathfrak{t}_{n-k}(1-\alpha/2)
\end{align*}
and the reported confidence interval is $[ \widehat{\Delta}_{jk} + \mathsf{se}(\widehat{\Delta}_{jk})\mathfrak{t}_{n-k}(\alpha/2), \widehat{\Delta}_{jk} + \mathsf{se}(\widehat{\Delta}_{jk})\mathfrak{t}_{n-k}(1-\alpha/2)]$.
]

---
class: title title-3
# Example

Consider the `arithmetic` data and the pairwise difference between praised (group C) and reproved (group D). 
.small[
- Sample average are $\widehat{\mu}_C = 27.4$ and $\widehat{\mu}_D = 23.4$, the pooled standard deviation is $1.15$
- The estimated average difference between groups $C$ and $D$ is $\widehat{\Delta}_{CD} = 4$.
- The standard error for the difference between groups is $\mathsf{se}(\widehat{\Delta}_{CD}) = 1.6216$
- If $\mathscr{H}_0: \Delta=0$, the $t$ statistic is $t=4/1.6216=2.467$.
- The critical values for a test at level $\alpha = 5$% are $-2.021$ and $2.021$ (`qt(0.975, df = 45 - 5)`)
- Since $|t| > 2.021$, reject $\mathscr{H}_0$: the two population are statistically significant at level $\alpha=5$%
- The confidence interval is [ $4-1.6216\times 2.021, 4+1.6216\times 2.021$ ] = [ $0.723, 7.28$ ]
- The $p$-value is $p=0.018$. We reject the null at level $\alpha=5$% since $0.018 < 0.05$.

]

---
class: title title-3

# Pairwise differences in **R**

```{r pairdiff, echo = TRUE, eval = FALSE}
library(tidyverse) # data manipulation
library(emmeans) # marginal means and contrasts
url <- "https://edsm.rbind.io/data/arithmetic.csv" 
# load data, define column type (factor and integer)
arithmetic <- read_csv(url, col_types = "fi") 
# fit one-way ANOVA model
model <- lm(score ~ group, data = arithmetic)
# Compute average of groups with model specification
margmeans <- emmeans::emmeans(model, specs = "group")
# Contrasts (default to pairwise comparisons) - no adjustment
contrast(margmeans, adjust = 'none', infer = TRUE) 
#infer = TRUE for confidence intervals
```
