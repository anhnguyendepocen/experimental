---
title: "One way ANOVA"
author: "Léo Belzile"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    lib_dir: "libs"
    chakra: "libs/remark-latest.min.js"
    css: ["default", "css/ath-slides.css", "css/ath-inferno-fonts.css", "css/animate.css"]
    seal: false
    anchor_sections: false
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
      navigation:
        scroll: false
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, 
                      fig.retina = 3, fig.align = "center")
```
```{r packages-data, echo = FALSE, include=FALSE}
library(tidyverse)
library(patchwork)
```
```{r xaringanExtra, echo=FALSE}
xaringanExtra::use_xaringan_extra(c("tile_view","freezeframe","panelset"))
# xaringanExtra::use_clipboard()
xaringanExtra::use_broadcast()
```

class: center middle main-title section-title-1

# One way ANOVA

.class-info[

**Session 3**

.light[MATH 80667A: Experimental Design and Statistical Methods <br>for Quantitative Research in Management <br>
HEC Montréal
]

]

---

name: outline
class: title title-inv-5

# Outline
--

.box-2.medium.sp-after-half[Recap]

--

.box-8.medium.sp-after-half[F tests]

--

.box-4.medium.sp-after-half[Parametrizations and interpretation]

--

.box-3.medium.sp-after-half[Planned comparisons and *post-hoc* tests]


---

layout: false
name: recap
class: center middle section-title section-title-2 animated fadeIn

# Refresher on hypothesis tests

---

class: title title-2

# General recipe of hypothesis testing

.pull-left.left-align[

.box-2.sp-after-half[
(1) Define variables
]

.box-2.sp-after-half[
(2) Write down hypotheses
]

.box-2.sp-after-half[
(3) Choose/compute a test statistic
]

.box-2.sp-after-half[
(4) Benchmark
]

]

.pull-right[

.box-2.sp-after-half[
(5) Compute the _p_-value
]
.box-inv-2.sp-after-half[
(6) Conclude (reject/fail to reject)
]
.box-2.sp-after-half[
(7) Report findings
]

]

---

class: title title-2

# Level

.box-2.sp-after-half[
Level = probability of judicial error 


Analyst fixes **level** $\alpha$
**before** the experiment.
]

.box-inv-2.sp-after-half[

Choose $\alpha$ as small as possible (don't condemn the innocent!)

(typical value is 5%)
]

.box-2.sp-after-half[
Reject $\mathscr{H}_0$ if p-value less than $\alpha$
]


???

Question: why can't we fix $\alpha=0$?



---

class: title title-2
# Errors


```{r nullvsalternative, eval = TRUE, echo = FALSE, cache = TRUE, fig.height=7, fig.width = 11, fig.align ='center'}
g1 <- ggplot() + 
  stat_function(xlim = c(0, 8), 
                n = 1001,
                fun=df, args = list(df1 = 4, df2 = 80, ncp = 0)) + 
  stat_function(geom = "area", fill = "blue", alpha = 0.2,
                xlim = c(qf(0.95, 4,80),8), 
                n = 1001,
                fun=df, args = list(df1 = 4, df2 = 80, ncp = 0)) + 
  labs(x = "statistic", 
       y = "density", 
       subtitle = "null is true") +  
  scale_y_continuous(expand = c(0, 0)) + 
  theme_classic()
g2 <- ggplot() + 
  stat_function(xlim = c(0, 8), 
                n = 1001,
                fun=df, args = list(df1 = 4, df2 = 80, ncp = 3)) + 
  stat_function(geom = "area", fill = "blue", alpha = 0.2,
                xlim = c(0, qf(0.95, 4,80)), 
                n = 1001,
                fun=df, args = list(df1 = 4, df2 = 80, ncp = 3)) +  
  labs(x = "statistic", 
       y = "density", subtitle = "alternative (null is false)") + scale_y_continuous(expand = c(0, 0)) + 
  theme_classic()
  
  
g1 + g2 
```

???

- Talk about cutoff values
- Discuss the tradeoff between type I and type II errors

---

class: title title-2

# Confidence intervals

Test statistics are standardized, 
- Good for comparisons with benchmark
- typically meaningless (standardized = unitless quantities)

Two options for reporting: 

- $p$-value: probability of more extreme outcome if no mean difference
- confidence interval: set of all values for which we fail to reject the null hypothesis at level $\alpha$ for the given sample

---
class: title title-2

# Interpretation of confidence intervals


.box-7.sp-after-half[ confidence interval = [lower, upper] units]

.small[
Interpretation: under the null, if we repeat the experiments 95%, of intervals will contain the true value (if model correctly calibrated)]


.box-7.sp-after-half[Analogy: coin toss]

```{r out.width = '50%', out.height = '40%', fig.alt = "100 confidence intervals", eval = TRUE, echo = FALSE}
knitr::include_graphics("img/03/confint.png")
```

---

layout: false
name: f-test
class: center middle section-title section-title-8 animated fadeIn

# One way analysis of variance

---

class: title title-8

# F-test for one way ANOVA

.box-inv-8.medium[Global null hypothesis]

No difference between treatments

- $\mathscr{H}_0$ (null): all of the $K$ treatment groups have the same average $\mu$
- $\mathscr{H}_a$ (alternative): at least two treatments have different averages

???

- The null hypothesis can be viewed as a special case from a bigger class of possibilities
- it always corresponds to some restrictions from the alternative class

---

class: title title-8

# _F_-test statistic

.box-8.medium[Omnibus test]

With $K$ groups and $n$ observations, the statistic is

\begin{align*}
F =  \frac{\text{between sum of squares}/(K-1)}{\text{within sum of squares}/(n-K)}
\end{align*}


The null distribution (benchmark) is $F(K-1, n-K)$.

---

class: title title-8

# Why does it work?

.small[
Denote
- $y_{ik}$ is observation $i$ of group $k$
- $\widehat{\mu}_1, \ldots, \widehat{\mu}_K$ the sample average of groups $1, \ldots, K$
- $\widehat{\mu}$ is overall sample mean

]

.box-8[Decomposing variability into bits]

\begin{align*}
\underset{\text{total sum of squares}}{\sum_{i}\sum_{k} (y_{ik} - \widehat{\mu})^2} &= \underset{\text{within sum of squares}}{\sum_i \sum_k (y_{ik} - \widehat{\mu}_k)^2} +  \underset{\text{between sum of squares}}{\sum_k n_i (\widehat{\mu}_k - \widehat{\mu})^2}.
\end{align*}

.pull-left-3[
.box-inv-8[null model]
]
.pull-middle-3[
.box-inv-8[alternative model]
]
.pull-right-3[
.box-inv-8[added variability]
]

---


class: title title-8

# Degrees of freedom

The parameters of the null distribution are called **degrees of freedom**

- $K-1$ is the number of constraints imposed by the null
- $n-K$ is the number of observations minus number of mean parameters estimated under alternative

Idea of _F_-statistic: under the null, both numerator and denominator are variance estimators.

- but the numerator is more variable...
- the $F$ ratio should be approximately one on average

---

layout: false
name: parametrization
class: center middle section-title section-title-4 animated fadeIn

# Parametrizations and interpretation

---

class: title title-4

# Parametrization 1: sample averages

Most natural parametrization, not useful for test

- Sample sizes in each group: $n_1, \ldots, n_K$, are known.

- sample average of each treatment group: $\widehat{\mu}_1, \ldots, \widehat{\mu}_K$.

.box-inv-4.sp-after-half[
$K$ means  = $K$ parameters
]

Overall mean is 
\begin{align*}
n \widehat{\mu} = n_1 \widehat{\mu}_1 + \cdots + n_K \widehat{\mu}_K
\end{align*}


---
class: title title-4

# Parametrization 2: contrasts

In terms of differences, relative to a baseline category $j$



- Intercept = sample mean $\widehat{\mu}_j$
- Coefficient for group $k \neq j$: $\widehat{\mu}_j - \widehat{\mu}_k$
    - difference between averages of baseline and group $k$

In **R**, the baseline is the smallest alphanumerical value.

.box-inv-4[
```{r echo = TRUE, eval = FALSE}
lm(response ~ group)
```
]

---
class: title title-4

# Parametrization 3: sum-to-zero

In terms of differences, relative to average of $\widehat{\mu}_1, \ldots, \widehat{\mu}_K$



- Intercept = $(\widehat{\mu}_1 + \cdots + \widehat{\mu}_K)/K$
- Coefficient for group $k$: intercept minus $\widehat{\mu}_k$

In **R**, the last factor level is dropped by default.

.box-inv-4[
```{r echo = TRUE, eval = FALSE}
lm(response ~ group, contrasts = contr.sum(group))
```
]

---
class: title title-4

# Comparison for the arithmetic example


<table>
 <thead>
  <tr>
   <th style="text-align:left;"> group </th>
   <th style="text-align:right;"> mean </th>
   <th style="text-align:right;"> contrasts </th>
   <th style="text-align:right;"> sum-to-zero </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:left;"> intercept </td>
   <td style="text-align:right;">  </td>
   <td style="text-align:right;"> 19.67 </td>
   <td style="text-align:right;"> 21.00 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> control 1 </td>
   <td style="text-align:right;"> 19.66 </td>
   <td style="text-align:right;">  </td>
   <td style="text-align:right;"> -1.33 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> control 2 </td>
   <td style="text-align:right;"> 18.33 </td>
   <td style="text-align:right;"> -1.33 </td>
   <td style="text-align:right;"> -2.66 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> praised </td>
   <td style="text-align:right;"> 27.44 </td>
   <td style="text-align:right;"> 7.77 </td>
   <td style="text-align:right;"> 6.44 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> reproved </td>
   <td style="text-align:right;"> 23.44 </td>
   <td style="text-align:right;"> 3.77 </td>
   <td style="text-align:right;"> 2.44 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> ignored </td>
   <td style="text-align:right;"> 16.11 </td>
   <td style="text-align:right;"> -3.55 </td>
   <td style="text-align:right;">  </td>
  </tr>
</tbody>
</table>

---
layout: false
name: planned-comparisons
class: center middle section-title section-title-3 animated fadeIn

# Planned comparisons and post-hoc tests

---

class: title title-3

# Planned comparisons

Oftentimes, we are not interested in the global null hypothesis.

- Can formulate planned comparisons *at registration time* for effects of interest

.box-inv-3.large[What is the scientific question of interest?]

---

class: title title-3

# Arithmetic example

.box-3.medium[Setup]
.pull-left-3[
.box-inv-3[
group 1 
]
.center[
(control)
]
]
.pull-middle-3[
.box-inv-3.sp-after-half[
group 2 
]
.center[
(control)
]
]

.pull-right-3[
.box-inv-3.sp-after-half[
group 3
]
.center[
(praise, reprove, ignore)
]
]

--

.box-3.medium[Hypothesis of interest]

- $\mathscr{H}_{01}$: $\mu_{\text{praise}} = \mu_{\text{reprove}}$ (attention)
- $\mathscr{H}_{02}$: $\frac{1}{2}(\mu_{\text{control}_1}+\mu_{\text{control}_2}) = \mu_{\text{praise}}$ (encouragement)

???

This is post-hoc, but supposed we had particular interest in the following hypothesis (for illustration purposes)

---

class: title title-3

# Contrasts

With placeholders for each group, write
$\mathscr{H}_{01}: \mu_{\text{praise}} = \mu_{\text{reprove}}$ as 
.small.center[

$0\cdot \mu_{\text{control}_1}$ + $0\cdot \mu_{\text{control}_2}$ + $1 \cdot \mu_{\text{praise}}$ - $1\cdot \mu_{\text{reprove}}$ + $0 \cdot \mu_{\text{ignore}}$

]

The sum of the coefficients, $(0, 0, 1, -1, 0)$, is zero.

.box-3[Contrast = sum-to-zero constraint]

--

Similarly, for $\mathscr{H}_{02}: \frac{1}{2}(\mu_{\text{control}_1}+\mu_{\text{control}_2}) = \mu_{\text{praise}}$
.small.center[

$\frac{1}{2} \cdot \mu_{\text{control}_1}$ + $\frac{1}{2}\cdot \mu_{\text{control}_2}$ - $1 \cdot \mu_{\text{praise}}$ + $0\cdot \mu_{\text{reprimand}}$ + $0 \cdot \mu_{\text{ignore}}$

]

The contrast vector $\left(\frac{1}{2}, \frac{1}{2}, -1, 0, 0\right)$ sums to zero.

Equivalent formulation is obtained by picking $(1,1,-2,0,0)$

---
class: title title-3

# Contrasts in **R**

```{r eval = FALSE, echo = TRUE}
library(emmeans)
linmod <- lm(score ~ group, data = arithmetic)
linmod_emm <- emmeans(linmod, specs = 'group')
contrast_specif <- list(
  controlvspraised = c(0.5, 0.5, -1, 0, 0),
  praisedvsreproved = c(0, 0, 1, -1, 0)
)
contrasts_res <- 
  contrast(object = linmod_emm, 
                    method = contrast_specif)
# Obtain confidence intervals instead of p-values
confint(contrasts_res)
```

---

---

class: title title-3

# *Post-hoc* tests

Maybe there is some difference between groups?

Unplanned comparisons: go fishing...

.box-inv-3[
Comparing all pairwise differences = $\binom{K}{2}$ tests]

With $K=5$ groups, we get 10 pairwise comparisons.

```{r eval = FALSE}
emmeans(modlin, pairwise ~ group)
```

If there were no differences between the groups, how many do we expect to find significant by chance with $\alpha = 0.1$?

---

class: title title-3

# Pairwise differences and _t_-tests

.box-inv-3.left[Technical aside]

The pairwise differences (_p_-values) and confidence intervals for groups $j$ and $k$ are based on the _t_-statistic:

\begin{align*}
t = \frac{(\widehat{\mu}_j - \widehat{\mu}_k) - 0}{\mathsf{se}(\widehat{\mu}_j - \widehat{\mu}_k)}
\end{align*}
which has a null $\mathcal{T}(n-k)$ distribution.

The standard error $\mathsf{se}(\widehat{\mu}_j - \widehat{\mu}_k)$ uses the pooled variance estimate, i.e., the within sum of squares divided by $n-K$


